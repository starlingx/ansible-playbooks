---
# ADMIN CREDENTIALS
# =================
#
# WARNING: It is strongly recommended to store these settings in Ansible vault
# file named "secret" under override files directory. Configuration parameters
# stored in vault must start with vault_ prefix (i.e. vault_admin_username,
# vault_admin_password).
#
admin_username: admin
admin_password: St8rlingXCloud*

# INITIAL PASSWORD CHANGE RESPONSE SEQUENCE
# =========================================
#
# The following two parameters are only relevant when the target host is bootstrapped
# remotely and the user wishes to change the initial sysadmin password as part of the
# bootstrap.
#
# WARNING: It is strongly recommended to store this setting in Ansible vault
# file named "secret" under override files directory. Configuration parameters
# stored in vault must start with vault_ prefix (i.e. vault_password_change_responses)
#
password_change: false

# The expected password strings differ between CentOS and Debian. The string
# "(current) UNIX password" is found only in CentOS, while the string "Current
# Password" is found only in Debian.
password_change_responses:
  yes/no: 'yes'
  sysadmin*: 'sysadmin'
  \(current\) UNIX password: 'sysadmin'
  Current password: 'sysadmin'
  (?i)New password: 'St8rlingXCloud*'
  (?i)Retype new password: 'St8rlingXCloud*'

# OVERRIDE FILES DIRECTORY
# ========================
#
# Default directory where user override file(s) can be found
#
override_files_dir: "{{ lookup('env', 'HOME') }}"

# BACKUP AND RESTORE
# ==================
#
# Location where the backup tar file is placed to perform the restore.
# This location must be specified at the command line via ansible-playbook -e option.
initial_backup_dir:

# This variable refers to the tar file that is generated by the backup
# procedure and used in the restore phase. The filename must be specified
# at the command line via ansible-playbook -e option.
backup_filename:

# Default directory where the backup tar file(s) can be found
# on the active controller
backup_dir: /opt/backups

# The staging directory to platform-backup to have a consistent behavior
# regardless of where the restore playbook is executed (locally vs remotely)
target_backup_dir: /opt/platform-backup

# Enable encryption of backup files.  The default is false until this
# feature is activated.
backup_encryption_enabled: false

# A passphrase with which to encrypt.  Required when
# backup_encryption_enabled is true
backup_encryption_passphrase: ""

# A list of identifiers indicating which backup files to encrypt:
#   [platform, openstack, user_images, dc_vault, registry, hc_vault, openbao]
backup_encyption_include:
  - platform
  - hc_vault
  - openbao

# Internal boolean variables for encryption to simplify logic.  These
# will be adjusted later when the overriden parameters above are
# considered.
platform_tarball_encrypted: false
hc_vault_tarball_encrypted: false
openbao_tarball_encrypted: false

# The platform backup tarball will be named in this format:
# <platform_backup_filename_prefix>_<timestamp>.tgz
#
platform_backup_filename_prefix: "{{ inventory_hostname }}_platform_backup"

# The local registry images backup tarball will be named in this format:
# <user_images_backup_filename_prefix>_<timestamp>.tgz
#
user_images_backup_filename_prefix: "{{ inventory_hostname }}_user_images_backup"

# The local image registry filesystem backup tarball will be named in this format:
# <registry_filesystem_backup_filename_prefix>_<timestamp>.tgz
#
registry_filesystem_backup_filename_prefix: "{{ inventory_hostname }}_image_registry_backup"

# The OpenStack application name that will be considered for backup
#
openstack_app_name: "stx-openstack"

# The OpenStack application backup tarball will be named in this format:
# <openstack_backup_filename_prefix>_<timestamp>.tgz
#
openstack_backup_filename_prefix: "{{ inventory_hostname }}_{{ openstack_app_name }}_backup"

# An indication whether it is a full restore or partial restore.
#   true:  a full restore where storage partition(s) is/are wiped during
#          platform restore and cinder+glance data needs restored
#   false: a partial restore where ceph data remain intact during restore
#
# This variable is used for StarlingX OpenStack application restore only
#

# The dc_vault backup tarball will be named in this format:
# <dc_vault_backup_filename_prefix>_<timestamp>.tgz
#
dc_vault_backup_filename_prefix: "{{ inventory_hostname }}_dc_vault_backup"

# This is the default value for including Hashicorp vault into the platform backup process.
# This value can be overridden by the user when calling for platform backup playbook,
# to include or not include the Hashicorp vault backup.
# If the Hashicorp vault application is either uploaded only or non-existent,
# the backup process will be omitted regardless of what this value is.
backup_hc_vault: false

# The hashicorp vault backup tarball will be named in this format:
# <hc_vault_backup_filename_prefix>_<timestamp>.tgz
#
hc_vault_backup_filename_prefix: "{{ inventory_hostname }}_hc_vault_backup"

# This is the default value for including openbao into the platform backup process.
# This value can be overridden by the user when calling for platform backup playbook,
# to include or not include the openbao backup.
# If the openbao application is either uploaded only or non-existent,
# the backup process will be omitted regardless of what this value is.
backup_openbao: false

# The openbao backup tarball will be named in this format:
# <openbao_backup_filename_prefix>_<timestamp>.tgz
#
openbao_backup_filename_prefix: "{{ inventory_hostname }}_openbao_backup"

restore_cinder_glance_data: false

# Default directory where the system backup tarballs fetched from the
# active controller can be found
#
host_backup_dir: "{{ lookup('env', 'HOME') }}"

# Flag file to indicate if platform restore is in progress
#
restore_in_progress_flag: /etc/platform/.restore_in_progress

# By default, restore cannot run if a restore is already in progress.
# By setting this to true, the restore_in_progress_flag will be ignored.
force_restore: false

# This variable is set to true when restore_openstack playbook is
# played again to bring up remaining OpenStack services after Ceph
# data is restored.
restore_openstack_continue: false

# When set to false, disk partitions that were previously used for Ceph data are
# not wiped. Otherwise, all disks are wiped as part of the bootstrap.
wipe_ceph_osds: false

# This variable refers to the ssl_ca certificate used in the restore phase
# The filename must be specified at the command line via ansible-playbook -e option.
ssl_ca_certificate_file:

# The following parameter indicates where the backup data file(s) reside,
# on the host itself (true) or off box (false).
on_box_data: true

# This MAC address, if supplied, will be used to replace the management MAC
# address on controller-0 in the case of node replacement in SX configurations
replacement_mgmt_mac: null

# failure message which will be used to raise the error to the upper playbooks.
failure_msg: ""

---
#
# Copyright (c) 2019-2024 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# ROLE DESCRIPTION:
#   This role is to validate and save host (non secure) config.
#

- name: Fail if any of the configured registry keys is unknown
  fail:
    msg: "Unknown registry key: '{{ item }}'. Acceptable keys are {{ known_registry_keys|join(', ') }} "
  when: not item in known_registry_keys
  with_items: "{{ user_defined_registry_keys }}"

# error check the password section of docker registries
# check password parameters before trying to hide the password
# we need to do that here as opposed to with the other docker registry
# stuff because of the debug log statement.
# we need to do this all before the debug log statement to not log passwords.
- name: Check k8s_registry credentials
  fail:
    msg: "k8s registry username and password must both be specified or not at all"
  when: (docker_registries[default_k8s_registry.url].username is defined and
         docker_registries[default_k8s_registry.url].password is not defined) or
        (docker_registries[default_k8s_registry.url].username is not defined and
         docker_registries[default_k8s_registry.url].password is defined)

- name: Check gcr_registry credentials
  fail:
    msg: "gcr registry username and password must both be specified or not at all"
  when: (docker_registries[default_gcr_registry.url].username is defined and
         docker_registries[default_gcr_registry.url].password is not defined) or
        (docker_registries[default_gcr_registry.url].username is not defined and
         docker_registries[default_gcr_registry.url].password is defined)

- name: Check quay_registry credentials
  fail:
    msg: "quay registry username and password must both be specified or not at all"
  when: (docker_registries[default_quay_registry.url].username is defined and
         docker_registries[default_quay_registry.url].password is not defined) or
        (docker_registries[default_quay_registry.url].username is not defined and
         docker_registries[default_quay_registry.url].password is defined)

- name: Check docker_registry credentials
  fail:
    msg: "docker registry username and password must both be specified or not at all"
  when: (docker_registries[default_docker_registry.url].username is defined and
         docker_registries[default_docker_registry.url].password is not defined) or
        (docker_registries[default_docker_registry.url].username is not defined and
         docker_registries[default_docker_registry.url].password is defined)

- name: Check elastic_registry credentials
  fail:
    msg: "elastic registry username and password must both be specified or not at all"
  when: (docker_registries[default_elastic_registry.url].username is defined and
         docker_registries[default_elastic_registry.url].password is not defined) or
        (docker_registries[default_elastic_registry.url].username is not defined and
         docker_registries[default_elastic_registry.url].password is defined)

- name: Check ghcr_registry credentials
  fail:
    msg: "ghcr registry username and password must both be specified or not at all"
  when: (docker_registries[default_ghcr_registry.url].username is defined and
         docker_registries[default_ghcr_registry.url].password is not defined) or
        (docker_registries[default_ghcr_registry.url].username is not defined and
         docker_registries[default_ghcr_registry.url].password is defined)

- name: Check registryk8s_registry credentials
  fail:
    msg: "registryk8s registry username and password must both be specified or not at all"
  when: (docker_registries[default_registryk8s_registry.url].username is defined and
         docker_registries[default_registryk8s_registry.url].password is not defined) or
        (docker_registries[default_registryk8s_registry.url].username is not defined and
         docker_registries[default_registryk8s_registry.url].password is defined)

- name: Check icr_registry credentials
  fail:
    msg: "icr registry username and password must both be specified or not at all"
  when: (docker_registries[default_icr_registry.url].username is defined and
         docker_registries[default_icr_registry.url].password is not defined) or
        (docker_registries[default_icr_registry.url].username is not defined and
         docker_registries[default_icr_registry.url].password is defined)

- name: Check defaults registry credentials
  fail:
    msg: "defaults registry username and password must both be specified or not at all"
  when: docker_registries['defaults'] is defined and
        ((docker_registries['defaults'].username is defined and
         docker_registries['defaults'].password is not defined) or
        (docker_registries['defaults'].username is not defined and
         docker_registries['defaults'].password is defined))

# create a copy of docker_registries without passwords for debug logging
- set_fact:
    docker_registries_with_secrets: "{{ docker_registries }}"

- set_fact:
    docker_registries: "{{ docker_registries | combine(hide_pw, recursive=true) }}"
  vars:
    hide_pw: "{ '{{ item.key }}': { 'password': 'secret' } }"
  with_dict: "{{ docker_registries }}"
  no_log: true

- debug:
    msg:
      - System mode is {{ system_mode }}
      - Timezone is {{ timezone }}
      - Distributed Cloud Role is {{ distributed_cloud_role }}
      - Region name is {{ region_name }}
      - DNS servers is {{ dns_servers }}
      - PXE boot subnet is {{ pxeboot_subnet }}
      - Management subnet is {{ management_subnet }}
      - Cluster host subnet is {{ cluster_host_subnet }}
      - Cluster pod subnet is {{ cluster_pod_subnet }}
      - Cluster service subnet is {{ cluster_service_subnet }}
      - OAM subnet is {{ external_oam_subnet }}
      - OAM gateway is {{ external_oam_gateway_address }}
      - OAM floating ip is {{ external_oam_floating_address }}
      - Management dynamic address allocation is {{ management_dynamic_address_allocation }}
      - Cluster host dynamic address allocation is {{ cluster_host_dynamic_address_allocation }}
      - Docker registries is {{ docker_registries }}
      - Docker HTTP proxy is {{ docker_http_proxy }}
      - Docker HTTPS proxy is {{ docker_https_proxy }}
      - Docker no proxy list is {{ docker_no_proxy }}
      - Applications are {{ applications }}

- debug:
    msg:
      - Admin subnet is {{ admin_subnet }}
  when: admin_network is defined

# System parameters config validation
- block:
  - name: Set system mode fact
    set_fact:
      system_mode: "{{ system_mode|lower }}"

  - block:
    - debug:
        msg: "System type is Standard, system mode will be set to duplex."
    - name: Set system mode to duplex for Standard system
      set_fact:
        system_mode: duplex
    when: system_type == 'Standard'

  - name: Validate system mode if system type is All-in-one
    fail:
      msg: "Invalid system mode. Valid values are: simplex, duplex or duplex-direct."
    when: >
      (system_mode != 'simplex' and
       system_mode != 'duplex' and
       system_mode != 'duplex-direct') and
      (system_type == 'All-in-one')

  - name: Validate virtual system setting
    fail:
      msg: "virtual_system is misconfigured. Valid value is either 'True' or 'False'."
    when: >
      (virtual_system is defined and
       not virtual_system | type_debug == 'bool')

  - name: Fail if virtual system type is not All-in-one
    fail:
      msg: "Virtual system setting is only supported for All-in-one system"
    when: >
      (virtual_system is defined and
       virtual_system|bool and
       system_type != 'All-in-one')

  - name: Validate distributed cloud role
    fail:
      msg: "Invalid distributed cloud role. Valid values are: none, systemcontroller, or subcloud."
    when: >
      (distributed_cloud_role != 'none' and
       distributed_cloud_role != 'systemcontroller' and
       distributed_cloud_role != 'subcloud')

  - block:
    - name: Validate system type and system mode if distributed cloud role is system controller
      fail:
        msg: "A simplex All-in-one controller cannot be configured as Distributed Cloud System Controller"
      when: system_mode == 'simplex'

    - name: Validate virtual system setting if distributed cloud role is system controller
      fail:
        msg: "Virtual system setting is not supported for Distributed Cloud System Controller"
      when: >
        (virtual_system is defined and
         virtual_system|bool)

    when: >
      (distributed_cloud_role == 'systemcontroller' and
       system_type == 'All-in-one')

  - name: Checking registered timezones
    stat:
      path: "{{ '/usr/share/zoneinfo/' + timezone }}"
    register: timezone_file

  - name: Fail if provided timezone is unknown
    fail: msg="The provided timezone {{ timezone }} is invalid."
    when: not timezone_file.stat.exists

  - name: Fail if the number of dns servers provided is not at least 1 and no more than 3
    fail:
      msg: "The number of DNS servers exceeds maximum allowable number of 3."
    when: (dns_servers | length == 0) or (dns_servers | length > 3)


# DNS servers config validation
- name: Check format of DNS server IP(s)
  debug:
    msg: "DNS Server: {{ item }}"
  failed_when: item | ipaddr == False
  with_items: "{{ dns_servers }}"

- name: Update resolv.conf with list of dns servers
  lineinfile:
    path: /etc/resolv.conf
    line: "nameserver {{ item }}"
  with_items: "{{ dns_servers }}"

# Networks config validation
- block:
  - name: Validate provided subnets (both IPv4 & IPv6 notations)
    debug:
      msg: "{{ item.key }}: {{ item.value }}"
    failed_when: item.value|ipaddr == False
    with_dict: "{{ network_params }}"

  - set_fact:
      ipv4_addressing: "{{ network_params.management_subnet|ipv4 }}"
      ipv6_addressing: "{{ network_params.management_subnet|ipv6 }}"

  - name: Validate all network subnets are IPv4
    debug:
      msg: "All infrastructure and cluster subnets must be the same IP version"
    failed_when: item|ipv4 == False
    with_items:
      - "{{ network_params.management_subnet }}"
      - "{{ network_params.cluster_host_subnet }}"
      - "{{ network_params.cluster_pod_subnet }}"
      - "{{ network_params.cluster_service_subnet }}"
      - "{{ network_params.external_oam_subnet }}"
      - "{{ network_params.management_multicast_subnet }}"
      - "{{ network_params.admin_subnet | default([]) }}"
    when: ipv4_addressing != False

  - name: Validate all network subnets are IPv6
    debug:
      msg: "All infrastructure and cluster subnets must be the same IP version"
    failed_when: item|ipv6 == False
    with_items:
      - "{{ network_params.management_subnet }}"
      - "{{ network_params.cluster_host_subnet }}"
      - "{{ network_params.cluster_pod_subnet }}"
      - "{{ network_params.cluster_service_subnet }}"
      - "{{ network_params.external_oam_subnet }}"
      - "{{ network_params.management_multicast_subnet }}"
      - "{{ network_params.admin_subnet | default([]) }}"
    when: ipv6_addressing != False

  - name: Validate pxeboot subnet is IPv4
    debug:
      msg: "pxeboot_subnet subnet must always be IPv4"
    failed_when: network_params.pxeboot_subnet|ipv4 == False

  - name: Generate warning if subnet prefix is not typical for Standard systems
    debug:
      msg: "WARNING: Subnet prefix of less than /24 is not typical. This will affect scaling of the system!"
    when: item|ipaddr('prefix')|int < typical_subnet_prefix and system_type == 'Standard'
    with_items:
      - "{{ network_params.pxeboot_subnet }}"
      - "{{ network_params.management_subnet }}"
      - "{{ network_params.cluster_host_subnet }}"
      - "{{ network_params.external_oam_subnet }}"
      - "{{ network_params.management_multicast_subnet }}"

  - block:
    - name: Fail if IPv6 prefix length is too short
      fail:
        msg: "IPv6 minimum prefix length is {{ minimum_prefix_length }}"
      when: network_params.management_subnet|ipaddr('prefix')|int < minimum_ipv6_prefix_length

    when: ipv6_addressing != False

  - name: Fail if management address allocation is misconfigured
    fail:
      msg: "management_dynamic_address_allocation is misconfigured. Valid value is either 'True' or 'False'."
    when: not management_dynamic_address_allocation | type_debug == 'bool'

  - name: Fail if cluster-host address allocation is misconfigured
    fail:
      msg: "cluster_host_dynamic_address_allocation is misconfigured. Valid value is either 'True' or 'False'."
    when: not cluster_host_dynamic_address_allocation | type_debug == 'bool'

  - name: Fail if management start or end address is not configured for System Controller
    fail:
      msg: >-
           management_start_address and management_end_address are required
           for System Controller as this configuration requires address space
           left for gateway address(es).
    when: >
      (distributed_cloud_role == 'systemcontroller' and
      (management_start_address == 'derived' or management_end_address == 'derived'))

  - name: Fail if multicast subnet is not in valid range
    fail:
      msg: "Invalid multicast subnet: {{ item.name }} -> {{ item.address }}"
    when: item.address | ipaddr('multicast') == None
    with_items:
      - { name: 'management_multicast_subnet', address: "{{ network_params.management_multicast_subnet }}" }

  # The provided subnets have passed validation, set the default addresses
  # based on the subnet values
  - name: Set default start and end addresses based on provided subnets
    set_fact:
      default_pxeboot_start_address: "{{ (pxeboot_subnet | ipaddr(1)).split('/')[0] }}"
      default_pxeboot_end_address: "{{ (pxeboot_subnet | ipaddr(-2)).split('/')[0] }}"
      default_management_start_address: "{{ (management_subnet | ipaddr(1)).split('/')[0] }}"
      default_management_end_address: "{{ (management_subnet | ipaddr(-2)).split('/')[0] }}"
      default_cluster_host_start_address: "{{ (cluster_host_subnet | ipaddr(1)).split('/')[0] }}"
      default_cluster_host_end_address: "{{ (cluster_host_subnet | ipaddr(-2)).split('/')[0] }}"
      default_cluster_pod_start_address: "{{ (cluster_pod_subnet | ipaddr(1)).split('/')[0] }}"
      default_cluster_pod_end_address: "{{ (cluster_pod_subnet | ipaddr(-2)).split('/')[0] }}"
      default_cluster_service_start_address: "{{ (cluster_service_subnet | ipaddr(1)).split('/')[0] }}"
      default_cluster_service_end_address: "{{ (cluster_service_subnet | ipaddr(-2)).split('/')[0] }}"
      default_external_oam_start_address: "{{ (external_oam_subnet | ipaddr(1)).split('/')[0] }}"
      default_external_oam_end_address: "{{ (external_oam_subnet | ipaddr(-2)).split('/')[0] }}"
      default_management_multicast_start_address: "{{ (management_multicast_subnet | ipaddr(1)).split('/')[0] }}"
      default_management_multicast_end_address: "{{ (management_multicast_subnet | ipaddr(-2)).split('/')[0] }}"
      default_external_oam_node_0_address: "{{ external_oam_floating_address | ipmath(1) }}"
      default_external_oam_node_1_address: "{{ external_oam_floating_address | ipmath(2) }}"

  - name: Set default start and end admin network addresses based on provided subnets
    set_fact:
      default_admin_start_address: "{{ (admin_subnet | ipaddr(1)).split('/')[0] }}"
      default_admin_end_address: "{{ (admin_subnet | ipaddr(-2)).split('/')[0] }}"
    when: admin_network is defined

  - name: Build address pairs for validation, merging default and user provided values
    set_fact:
      address_pairs:
        pxeboot:
          start:
            "{{ pxeboot_start_address if pxeboot_start_address != 'derived'
            else default_pxeboot_start_address }}"
          end:
            "{{ pxeboot_end_address if pxeboot_end_address != 'derived'
            else default_pxeboot_end_address }}"
          subnet: "{{ network_params.pxeboot_subnet }}"
        management:
          start:
            "{{ management_start_address if management_start_address != 'derived'
            else default_management_start_address }}"
          end:
            "{{ management_end_address if management_end_address != 'derived'
            else default_management_end_address }}"
          subnet: "{{ network_params.management_subnet }}"
        cluster_host:
          start:
            "{{ cluster_host_start_address if cluster_host_start_address != 'derived'
            else default_cluster_host_start_address }}"
          end:
            "{{ cluster_host_end_address if cluster_host_end_address != 'derived'
            else default_cluster_host_end_address}}"
          subnet: "{{ network_params.cluster_host_subnet }}"
        cluster_pod:
          start:
            "{{ cluster_pod_start_address if cluster_pod_start_address != 'derived'
            else default_cluster_pod_start_address }}"
          end:
            "{{ cluster_pod_end_address if cluster_pod_end_address != 'derived'
            else default_cluster_pod_end_address }}"
          subnet: "{{ network_params.cluster_pod_subnet }}"
        cluster_service:
          start:
            "{{ cluster_service_start_address if cluster_service_start_address != 'derived'
            else default_cluster_service_start_address }}"
          end:
            "{{ cluster_service_end_address if cluster_service_end_address != 'derived'
            else default_cluster_service_end_address }}"
          subnet: "{{ network_params.cluster_service_subnet }}"
        oam:
          start:
            "{{ external_oam_start_address if external_oam_start_address != 'derived'
            else default_external_oam_start_address }}"
          end:
            "{{ external_oam_end_address if external_oam_end_address != 'derived'
            else default_external_oam_end_address }}"
          subnet: "{{ network_params.external_oam_subnet }}"
        multicast:
          start:
            "{{ management_multicast_start_address if management_multicast_start_address != 'derived'
            else default_management_multicast_start_address }}"
          end:
            "{{ management_multicast_end_address if management_multicast_end_address != 'derived'
            else default_management_multicast_end_address }}"
          subnet: "{{ network_params.management_multicast_subnet }}"
        oam_node:
          start:
            "{{ external_oam_node_0_address if external_oam_node_0_address != 'derived'
            else default_external_oam_node_0_address }}"
          end:
            "{{ external_oam_node_1_address if external_oam_node_1_address != 'derived'
            else default_external_oam_node_1_address }}"
          subnet: "{{ network_params.external_oam_subnet }}"

  - name: Build admin address pairs for validation, merging default and user provided values
    set_fact:
      admin_pairs:
        start:
          "{{ admin_start_address if admin_start_address != 'derived'
          else default_admin_start_address }}"
        end:
          "{{ admin_end_address if admin_end_address != 'derived'
          else default_admin_end_address }}"
        subnet: "{{ network_params.admin_subnet }}"
    when: admin_network is defined

  - name: Add admin address pairs to address pairs
    set_fact:
      address_pairs: "{{ address_pairs | combine({'admin': admin_pairs}) }}"
    when: admin_network is defined

  - include: validate_address_range.yml
    with_dict: "{{ address_pairs }}"

  - name: Set OAM address list
    set_fact:
      OAM_addresses: "{{ [external_oam_floating_address] }}"

  - name: Update OAM address list for duplex
    set_fact:
      OAM_addresses: "{{ OAM_addresses + [ address_pairs['oam_node']['start'], address_pairs['oam_node']['end'] ] }}"
    when: system_mode != 'simplex'

  - name: Set floating addresses based on subnets or start addresses
    set_fact:
      # Not sure why ipaddr('address') and ipsubnet filter did not extract
      # the IP from CIDR input. Resort to string split for now.
      controller_floating_address: "{{ address_pairs['management']['start'] }}"
      controller_pxeboot_floating_address: "{{ address_pairs['pxeboot']['start'] }}"
      cluster_floating_address: "{{ address_pairs['cluster_host']['start'] }}"

  - name: Set subcloud admin floating address
    set_fact:
      controller_admin_floating_address: "{{ address_pairs['admin']['start'] }}"
    when: address_pairs.admin is defined

  - name: Set subcloud floating address
    set_fact:
      sc_floating_address: "{{ controller_admin_floating_address
                           if controller_admin_floating_address is defined
                           else address_pairs.management.start }}"
    when: distributed_cloud_role == 'subcloud'

  - name: Set derived facts for subsequent tasks/roles
    set_fact:
      derived_network_params:
        'management_interface': lo
        'management_interface_name': lo
        'controller_0_address': "{{ controller_floating_address|ipmath(1) }}"
        'controller_1_address': "{{ controller_floating_address|ipmath(2) }}"
        'controller_pxeboot_address_0': "{{ controller_pxeboot_floating_address|ipmath(1) }}"
        'controller_pxeboot_address_1': "{{ controller_pxeboot_floating_address|ipmath(2) }}"

      # Make common facts available to other roles
      config_workdir: "{{ config_workdir }}"
      dns_servers: "{{ dns_servers }}"

      # Derived network parameters that don't apply to bootstrap_config but are required for
      # subsequent roles
      management_subnet_prefix: "{{ management_subnet | ipaddr('prefix') }}"
      management_broadcast: "{{ management_subnet | ipaddr('broadcast') }}"
      pxe_subnet_prefix: "{{ pxeboot_subnet | ipaddr('prefix') }}"
      cluster_subnet_prefix: "{{ cluster_host_subnet | ipaddr('prefix') }}"
      cluster_broadcast: "{{ cluster_host_subnet | ipaddr('broadcast') }}"
      controller_0_cluster_host: "{{ cluster_floating_address|ipmath(1) }}"
      controller_1_cluster_host: "{{ cluster_floating_address|ipmath(2) }}"
      controller_floating_address_url:
        "{{ controller_floating_address|ipwrap if controller_floating_address|ipv6 != False
        else controller_floating_address}}"

  - name: Set facts for IP address provisioning against loopback interface
    set_fact:
      mgmt_virtual: "{{ derived_network_params.controller_0_address }}/{{ management_subnet_prefix }}"
      cluster_virtual: "{{ controller_0_cluster_host }}/{{ cluster_subnet_prefix }}"
      pxe_virtual: "{{ controller_pxeboot_floating_address }}/{{ pxe_subnet_prefix }}"
      cluster_floating_virtual: "{{ cluster_floating_address }}/{{ cluster_subnet_prefix }}"
      mgmt_floating_virtual: "{{ controller_floating_address }}/{{ management_subnet_prefix }}"

  - name: Set facts for admin IP address provisioning against loopback interface
    set_fact:
      admin_broadcast: "{{ admin_subnet | ipaddr('broadcast') }}"
      admin_virtual: "{{ controller_admin_floating_address }}/{{ admin_subnet | ipaddr('prefix') }}"
    when: admin_network is defined

# Docker config validation
- block:
  - set_fact:
      use_default_registries: true
      k8s_registry: "{{ default_k8s_registry }}"
      gcr_registry: "{{ default_gcr_registry }}"
      quay_registry: "{{ default_quay_registry }}"
      docker_registry: "{{ default_docker_registry }}"
      elastic_registry: "{{ default_elastic_registry }}"
      ghcr_registry: "{{ default_ghcr_registry }}"
      registryk8s_registry: "{{ default_registryk8s_registry }}"
      icr_registry: "{{ default_icr_registry }}"
      default_no_proxy:
        - localhost
        - 127.0.0.1
        - registry.local
        - "{{ cluster_service_start_address if cluster_service_start_address != 'derived'
            else default_cluster_service_start_address }}"
        - "{{ controller_floating_address }}"
        - "{{ derived_network_params.controller_0_address }}"
        - "{{ external_oam_floating_address }}"
        - "{{ address_pairs['oam_node']['start'] }}"
      non_sx_proxy_addons:
        - "{{ derived_network_params.controller_1_address }}"
        - "{{ address_pairs['oam_node']['end'] }}"
      docker_no_proxy_combined: []

  - block:
    - name: Set subcloud no proxy list
      set_fact:
        subcloud_no_proxy:
          - registry.central
          - "{{ system_controller_oam_floating_address }}"
    - name: Update default no proxy list for subcloud
      set_fact:
        default_no_proxy: "{{ default_no_proxy + subcloud_no_proxy }}"
    when: distributed_cloud_role == 'subcloud'

  - block:
    - name: Set default no-proxy address list (non simplex)
      set_fact:
        default_no_proxy: "{{ default_no_proxy + non_sx_proxy_addons }}"
      when: system_mode != 'simplex'

    - block:
      - name: Validate http proxy urls
        include: validate_url.yml input_url={{ item }}
        with_items:
          - "{{ docker_http_proxy }}"
          - "{{ docker_https_proxy }}"

    - block:
      - name: Validate no proxy addresses
        include: validate_address.yml address_list={{ docker_no_proxy }}
        when: docker_no_proxy|length > 0

    - name: Add user defined no-proxy address list to default
      set_fact:
        docker_no_proxy_combined: "{{ default_no_proxy | union(docker_no_proxy) | ipwrap | unique }}"

    when: use_docker_proxy

  - block:
    - name: Turn on use_defaults_registry flag
      set_fact:
        use_defaults_registry: true
        k8s_registry: "{{ k8s_registry | combine(docker_registries_with_secrets['defaults'], recursive=true) }}"
        gcr_registry: "{{ gcr_registry | combine(docker_registries_with_secrets['defaults'], recursive=true) }}"
        quay_registry: "{{ quay_registry | combine(docker_registries_with_secrets['defaults'], recursive=true) }}"
        docker_registry: "{{ docker_registry | combine(docker_registries_with_secrets['defaults'], recursive=true) }}"
        elastic_registry: "{{ elastic_registry | combine(docker_registries_with_secrets['defaults'], recursive=true) }}"
        ghcr_registry: "{{ ghcr_registry | combine(docker_registries_with_secrets['defaults'], recursive=true) }}"
        registryk8s_registry: "{{ registryk8s_registry | combine(docker_registries_with_secrets['defaults'],
                              recursive=true) }}"
        icr_registry: "{{ icr_registry | combine(docker_registries_with_secrets['defaults'], recursive=true) }}"

    when: docker_registries['defaults'] is defined and docker_registries['defaults'] is not none

  - set_fact:
      k8s_registry:
        "{{ k8s_registry | combine(docker_registries_with_secrets[default_k8s_registry.url], recursive=true) }}"
    when: (docker_registries[default_k8s_registry.url]['url'] is not defined or
           (docker_registries[default_k8s_registry.url]['url'] is not none and
           docker_registries[default_k8s_registry.url]['url'] != default_k8s_registry.url))

  - set_fact:
      gcr_registry:
        "{{ gcr_registry | combine(docker_registries_with_secrets[default_gcr_registry.url], recursive=true) }}"
    when: (docker_registries[default_gcr_registry.url]['url'] is not defined or
           (docker_registries[default_gcr_registry.url]['url'] is not none and
           docker_registries[default_gcr_registry.url]['url'] != default_gcr_registry.url))

  - set_fact:
      quay_registry:
        "{{ quay_registry | combine(docker_registries_with_secrets[default_quay_registry.url], recursive=true) }}"
    when: (docker_registries[default_quay_registry.url]['url'] is not defined or
           (docker_registries[default_quay_registry.url]['url'] is not none and
           docker_registries[default_quay_registry.url]['url'] != default_quay_registry.url))

  - set_fact:
      docker_registry:
        "{{ docker_registry | combine(docker_registries_with_secrets[default_docker_registry.url], recursive=true) }}"
    when: (docker_registries[default_docker_registry.url]['url'] is not defined or
           (docker_registries[default_docker_registry.url]['url'] is not none and
           docker_registries[default_docker_registry.url]['url'] != default_docker_registry.url))

  - set_fact:
      elastic_registry:
        "{{ elastic_registry | combine(docker_registries_with_secrets[default_elastic_registry.url], recursive=true) }}"
    when: (docker_registries[default_elastic_registry.url]['url'] is not defined or
           (docker_registries[default_elastic_registry.url]['url'] is not none and
           docker_registries[default_elastic_registry.url]['url'] != default_elastic_registry.url))

  - set_fact:
      ghcr_registry:
        "{{ ghcr_registry | combine(docker_registries_with_secrets[default_ghcr_registry.url], recursive=true) }}"
    when: (docker_registries[default_ghcr_registry.url]['url'] is not defined or
           (docker_registries[default_ghcr_registry.url]['url'] is not none and
           docker_registries[default_ghcr_registry.url]['url'] != default_ghcr_registry.url))

  - set_fact:
      registryk8s_registry:
        "{{ registryk8s_registry | combine(docker_registries_with_secrets[default_registryk8s_registry.url],
        recursive=true) }}"
    when: (docker_registries[default_registryk8s_registry.url]['url'] is not defined or
           (docker_registries[default_registryk8s_registry.url]['url'] is not none and
           docker_registries[default_registryk8s_registry.url]['url'] != default_registryk8s_registry.url))

  - set_fact:
      icr_registry:
        "{{ icr_registry | combine(docker_registries_with_secrets[default_icr_registry.url], recursive=true) }}"
    when: (docker_registries[default_icr_registry.url]['url'] is not defined or
           (docker_registries[default_icr_registry.url]['url'] is not none and
           docker_registries[default_icr_registry.url]['url'] != default_icr_registry.url))

  - name: Update use_default_registries flag
    set_fact:
      use_default_registries: false
    when: use_defaults_registry or
          docker_registries|length != 5 or
          k8s_registry != default_k8s_registry or
          gcr_registry != default_gcr_registry or
          quay_registry != default_quay_registry or
          docker_registry != default_docker_registry or
          elastic_registry != default_elastic_registry or
          ghcr_registry != default_ghcr_registry or
          registryk8s_registry != default_registryk8s_registry or
          icr_registry != default_icr_registry

  - block:
    - name: Validate registry type if specified
      fail:
        msg: "Registry type for {{ item.key }} is not supported. Valid value is either 'docker' or 'aws-ecr'."
      with_dict: "{{ docker_registries }}"
      when: (item.value.type is defined and
            item.value.type not in ['docker', 'aws-ecr'])

    - name: Fail if secure registry flag is misconfigured
      fail:
        msg: "'secure' parameter of registry {{ item.key }} is misconfigured. Valid value is either 'True' or 'False'."
      with_dict: "{{ docker_registries }}"
      when: (item.value.secure is defined and
            not (item.value.secure|type_debug == 'bool'))

    - include: validate_address.yml address_list={{ addr_list }}
      vars:
        addr_list:
          - "{{ k8s_registry.url }}"
          - "{{ gcr_registry.url }}"
          - "{{ quay_registry.url }}"
          - "{{ docker_registry.url }}"
          - "{{ elastic_registry.url }}"
          - "{{ ghcr_registry.url }}"
          - "{{ registryk8s_registry.url }}"
          - "{{ icr_registry.url }}"

    when: not use_default_registries

- name: Validate additional_local_registry_images list
  fail:
    msg: "additional_local_registry_images must be a list"
  when: additional_local_registry_images | type_debug != 'list'

- import_role:
    role: common/validate-image-archives

# Validate user dns host records if defined
- block:
  - set_fact:
      user_dns_host_records_lines: ""

  - name: Validate and create entries for /etc/hosts from user dns host records
    script: parse_user_dns_host_record.py "{{ user_dns_host_records }}"
    register: result
    failed_when: false

  - name: Fail if script parse_user_dns_host_record.py failed or error occurred
    fail:
      msg: >
        Error parsing user dns host-records, return code: {{ result.rc }}
        Error output: {{ result.stderr }}
    when: result.rc != 0


  - name: Set entries for /etc/hosts in variable later use
    set_fact:
      user_dns_host_records_lines: "{{ result.stdout_lines }}"

  when: user_dns_host_records

# System applications validation
- name: Validate applications
  include: validate_application.yml application={{ item }}
  with_items: "{{ applications }}"
  when: applications

- name: Build application list
  set_fact:
    all_applications: []

- name: Append applications to application list
  set_fact:
    all_applications: "{{ all_applications }} + [ '{{ item.keys()|list|first }}' ]"
  with_items: "{{ applications }}"

- name: Get the name of the nginx tarball
  find:
    paths: "/usr/local/share/applications/helm/"
    patterns: 'nginx-ingress-controller-[^-]*-[^-]*\.tgz'
    use_regex: yes
  register: find_nginx_tarball_output

- name: Get the name of the cert manager tarball
  find:
    paths: "/usr/local/share/applications/helm/"
    patterns: 'cert-manager-[^-]*-[^-]*\.tgz'
    use_regex: yes
  register: find_cert_manager_tarball_output

# we prepend nginx and append cert manager to try and enforce ordering
# nginx need to be applied before cert manager
- name: Append default nginx entry if not present
  set_fact:
    applications: "[ {'{{ item.path }}': None}] + {{ applications }}"
  with_items: "{{ find_nginx_tarball_output.files }}"
  when: item.path not in all_applications

- name: Append default cert manager entry if not present
  set_fact:
    applications: "{{ applications }} + [ {'{{ item.path }}': None}]"
  with_items: "{{ find_cert_manager_tarball_output.files }}"
  when: item.path not in all_applications

- block:
  - name: Retrieve list of applications from sysinv
    shell: "source /etc/platform/openrc; system application-list --nowrap | awk '{print $2}'"
    register: application_list_output
    failed_when: false

  - name: Build list of existing applications
    set_fact:
      old_application_list: "{{ application_list_output.stdout.splitlines() }}"

  # TODO: modify system application-list so this is no longer needed
  # refer to system host-list's column and format options
  # the output of system application-list has 3 lines of header
  # ------
  # application
  # -----
  # get rid of that in our output that we captured
  - name: Remove header from application list output
    set_fact:
      old_application_list: "{{ old_application_list[3:] }}"

  # we need to purge old applications here because the replay might involve a teardown
  # of the kubernetes cluster. This would result in sysinv being unable to clean up
  # the deployed applications on its side
  - block:
    - name: Purge old applications
      include: purge_application.yml application={{ item }}
      with_items: "{{ old_application_list }}"

    # we check the return code from sysinv to see that application-list ran successfully
    # in a non replay, or weird replay cases, sysinv could be not up at which point
    # there is no point in trying to purge applications
    when: application_list_output.rc == 0 and old_application_list | length > 0
  when: replayed

- name: Validate apiserver_cert_sans list
  fail:
    msg: "apiserver_cert_sans must be a list"
  when: apiserver_cert_sans | type_debug != 'list'

- name: Validate apiserver_cert_sans entries
  include: validate_address.yml address_list={{ apiserver_cert_sans | list }}

- name: Check if external certificates and CA files exists
  block:
  - name: Verify that either both etcd root ca cert and key are defined or not at all
    fail:
      msg: "etcd_root_ca_cert and etcd_root_ca_key must be provided as a pair"
    when: (etcd_root_ca_cert and not etcd_root_ca_key) or
          (not etcd_root_ca_cert and etcd_root_ca_key)

  - name: Check if external CA files exist
    stat:
      path: "{{ lookup('vars', cert_override, default='') }}"
    register: external_ca_files
    with_items: "{{ external_cert_overrides_list }}"
    loop_control:
      loop_var: cert_override

  - name: Register if external CA files exist
    set_fact:
      ca_file_exist: >-
        {{ ca_file_exist | default ({}) |
        combine({
          cert_override[1] :
          external_ca_files.results[cert_override[0]].stat.exists
        }) }}
    with_indexed_items: "{{ external_cert_overrides_list }}"
    loop_control:
      loop_var: cert_override

  - name: Check for etcd_root_ca_cert file
    fail:
      msg: "etcd_root_ca_cert file not found. ({{ etcd_root_ca_cert }})"
    when: etcd_root_ca_cert and not ca_file_exist['etcd_root_ca_cert']

  - name: Check for etcd_root_ca_key file
    fail:
      msg: "etcd_root_ca_key file not found. ({{ etcd_root_ca_key }})"
    when: etcd_root_ca_key and not ca_file_exist['etcd_root_ca_key']

  - name: Verify that either both Kubernetes root ca cert and key are defined or not at all
    fail:
      msg: "k8s_root_ca_cert and k8s_root_ca_key must be provided as a pair"
    when: (k8s_root_ca_cert and not k8s_root_ca_key) or
          (not k8s_root_ca_cert and k8s_root_ca_key)

  - name: Check for k8s_root_ca_cert file
    fail:
      msg: "k8s_root_ca_cert file not found. ({{ k8s_root_ca_cert }})"
    when: k8s_root_ca_cert and not ca_file_exist['k8s_root_ca_cert']

  - name: Check for k8s_root_ca_key file
    fail:
      msg: "k8s_root_ca_key file not found. ({{ k8s_root_ca_key }})"
    when: k8s_root_ca_key and not ca_file_exist['k8s_root_ca_key']
  when: mode != 'restore'

- name: Verify 'system-local-ca' overrides
  block:
  - name: Check if the user provided file paths for RCA, ICA cert and ICA key
    set_fact:
      system_local_ca_rca_crt_provided: "{{ system_root_ca_cert | length > 0 }}"
      system_local_ca_ica_crt_provided: "{{ system_local_ca_cert | length > 0 }}"
      system_local_ca_ica_key_provided: "{{ system_local_ca_key | length > 0 }}"

  - name: Fail if any 'system-local-ca' override field is defined during upgrade
    fail:
      msg: >-
        ERROR: 'system-local-ca' override fields cannot be used during upgrades.
        Please remove all of the following from the override file: system_root_ca_cert,
        system_local_ca_cert, system_local_ca_key and system_platform_certificate.
        The procedure to update platform certificates (previously known as
        cert-manager migration) should be followed to change these certificates
        when necessary.
    when:
      - upgrade_in_progress
      - system_local_ca_rca_crt_provided or
        system_local_ca_ica_crt_provided or
        system_local_ca_ica_key_provided or
        system_platform_certificate is defined

  - name: Fail if 'system-local-ca' certificates overrides are present in subcloud
    fail:
      msg: >-
        ERROR: For subclouds, the data for 'system-local-ca' is propagated from the
        SystemController. Please remove all of the following from the override file:
        system_root_ca_cert, system_local_ca_cert and system_local_ca_key.
        You can stil provide custom overrides for the child platform certificates
        using the field 'system_platform_certificate'.

    when:
      - distributed_cloud_role == 'subcloud'
      - system_local_ca_rca_crt_provided or
        system_local_ca_ica_crt_provided or
        system_local_ca_ica_key_provided

  - name: Verify user provided data for system-local-ca
    block:
    - name: If one is provided, fail if user don't provided all the 3 fields
      fail:
        msg: >-
          "To set the local issuer, system_root_ca_cert, system_local_ca_cert and
          system_local_ca_key must be provided."
      when:
        - system_local_ca_rca_crt_provided or
          system_local_ca_ica_crt_provided or
          system_local_ca_ica_key_provided
        - not (system_local_ca_rca_crt_provided and
               system_local_ca_ica_crt_provided and
               system_local_ca_ica_key_provided)

    - name: Check the existence of system_root_ca_cert file
      fail:
        msg: "system_root_ca_cert file not found. ({{ system_root_ca_cert }})"
      when: (system_root_ca_cert | length > 0) and not ca_file_exist['system_root_ca_cert']

    - name: Check the existence of system_local_ca_cert file
      fail:
        msg: "system_local_ca_cert file not found. ({{ system_local_ca_cert }})"
      when: (system_local_ca_cert | length > 0) and not ca_file_exist['system_local_ca_cert']

    - name: Check the existence of system_local_ca_key file
      fail:
        msg: "system_local_ca_key file not found. ({{ system_local_ca_key }})"
      when: (system_local_ca_key | length > 0) and not ca_file_exist['system_local_ca_key']

    - name: Encode the user provided cert/key files
      block:
        - name: Encode system_root_ca_cert
          shell: cat "{{ system_root_ca_cert }}" | base64 -w0
          register: root_ca_cert_output
          no_log: true

        - name: Encode system_local_ca_cert
          shell: cat "{{ system_local_ca_cert }}" | base64 -w0
          register: local_ca_cert_output
          no_log: true

        - name: Encode system_local_ca_key
          shell: cat "{{ system_local_ca_key }}" | base64 -w0
          register: local_ca_key_output
          no_log: true

        - name: Keep the file paths for the CA certs/key as variables
          block:
            - set_fact:
                system_root_ca_cert_file: "{{ system_root_ca_cert }}"
                system_local_ca_cert_file: "{{ system_local_ca_cert }}"
                system_local_ca_key_file: "{{ system_local_ca_key }}"

            - set_fact:
                system_root_ca_cert: "{{ root_ca_cert_output.stdout }}"
                system_local_ca_cert: "{{ local_ca_cert_output.stdout }}"
                system_local_ca_key: "{{ local_ca_key_output.stdout }}"
                system_local_ca_overrides: true
              no_log: true

        - name: Verify 'system-local-ca' certs
          include_role:
            name: common/verify-and-install-system-local-ca-certs
          vars:
            - install_rca: false
            - enforce_ica: true

      when: system_local_ca_cert | length > 0
    when: mode != 'restore'

- name: Verify user provided SANs for platform certificates
  include_role:
    name: common/validate-cert-subject-fields
  when: system_platform_certificate is defined

- name: Check for ssl_ca_cert file
  fail:
    msg: "ssl_ca_cert file not found. ({{ ssl_ca_cert }})"
  when: mode != 'restore' and ssl_ca_cert is defined and not ca_file_exist['ssl_ca_cert']

- name: Check OpenID Connect parameters
  fail:
    msg: "If OpenID Connect parameters are specified, you must specify all 3
          apiserver_oidc: client_id, issuer_url, username_claim or
          apiserver_oidc: groups_claim in addition to the previous 3."
  when: not ((apiserver_oidc|length == 0)
        or
        ((apiserver_oidc|length == 3) and
        apiserver_oidc.client_id is defined and
        apiserver_oidc.issuer_url is defined and
        apiserver_oidc.username_claim is defined)
        or
        ((apiserver_oidc|length == 4) and
        apiserver_oidc.client_id is defined and
        apiserver_oidc.issuer_url is defined and
        apiserver_oidc.username_claim is defined and
        apiserver_oidc.groups_claim is defined))

- name: Check overridden kubernetes_version matches the backup.
  fail:
    msg: >
      Cannot override kubernetes_version {{ kubernetes_version }}
      different than value {{ restore_k8s_version }} in backup tarball!
  when: (kubernetes_version is defined) and
        (restore_k8s_version is defined) and
        (kubernetes_version != restore_k8s_version)

- name: Check extraVolumes configuration.
  fail:
    msg: >
      Attribute is missing in extra volume {{ item.name }}.
      Or readOnly attribute's value is not 'false' or 'true'
      Or pathType attribute's value is not 'File' or 'DirectoryOrCreate'
  when: (item.name is not defined) or
        (item.mountPath is not defined) or
        (item.hostPath is not defined) or
        (item.pathType is not defined) or
        (item.pathType != 'File' and item.pathType != 'DirectoryOrCreate') or
        (item.readOnly is not defined) or
        (item.readOnly | type_debug != 'bool')
  loop: "{{ apiserver_extra_volumes + controllermanager_extra_volumes + scheduler_extra_volumes }}"

- import_role:
    name: common/wipe-ceph-osds

# bootstrap_config ini file generation
- block:
  # A previous config workdir can exist if the playbook failed or
  # was aborted before moving the contents and deleting the directory
  # on the persist-config role. If the config ini file still exists
  # before generating a new one, duplicate keys can appear, resulting
  # in failure during "Saving config in sysinv database".
  - name: Delete previous config workdir if exists
    file:
      path: "{{ config_workdir }}"
      state: absent

  - name: Create config workdir
    file:
      path: "{{ config_workdir }}"
      state: directory
      owner: root
      group: root
      mode: 0755

  - name: Write simplex flag
    file:
      path: /etc/platform/simplex
      state: touch

  when: save_config_to_db and restore_mode|default(none) != 'optimized'

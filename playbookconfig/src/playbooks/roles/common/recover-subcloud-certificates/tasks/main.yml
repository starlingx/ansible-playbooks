---
#
# Copyright (c) 2023-2025 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# ROLE DESCRIPTION:
#   This role performs certificate recovery for subclouds when offline for
#   a long period of time
#


- name: Source platform.conf and get variables
  shell: |
    source /etc/platform/platform.conf
    printf '{"system_mode": "%s", "nodetype": "%s", "sw_version": "%s"}' \
      "$system_mode" "$nodetype" "$sw_version"
  register: platform_conf_result

- set_fact:
    system_mode_result: "{{ platform_dict.system_mode }}"
    node_type_result: "{{ platform_dict.nodetype }}"
    sw_version_result: "{{ platform_dict.sw_version }}"
  vars:
    platform_dict: "{{ platform_conf_result.stdout | from_json }}"

- set_fact:
    is_controller: "{{ true if node_type_result == 'controller' else false }}"
    is_compute: "{{ true if node_type_result == 'worker' else false }}"

# When source_env_command is an empty string, we already sourced credentials
# before, so we already know we're on the active controller and don't need to
# check again
- name: Check if current controller is the active controller
  block:
  - name: Check if admin credentials can be sourced
    shell: source /etc/platform/openrc
    failed_when: false
    register: active_controller

  - set_fact:
      is_active_controller: >-
        {{ true if 'stdout' in active_controller and active_controller.stdout
        | length == 0 else false }}

  when: is_controller and (source_env_command is defined and source_env_command | length > 0)

- name: Verify certificate expiration status
  import_tasks: verify-certificate-expiration-status.yml
  when: is_active_controller

- block:
  - name: Recover K8s Root CA certificates (K8s Root CA, ETCD CA, FrontProxy CA)
    import_tasks: recover-k8s-root-cas.yml

  - name: Renew K8s leaf certificates
    import_tasks: recover-k8s-leaf-certificates.yml

  - block:
    - name: Recover dc admin endpoint Root CA, subcloud ICA and leaf certificates
      import_tasks: recover-dc-admin-ep-certificate-chain.yml
      # dc certificates are set up later in the enrollment playbook, so there's
      # nothing to recover at the moment if running from the enrollment playbook
      when: mode is not defined or mode != "enroll"

    - name: Validate rest-api / docker registry certificates
      import_tasks: validate-restapi-and-registry-certificates.yml

    when: is_active_controller

  - name: Do certificate recovery on other nodes when system type is multi-node
    block:
      - name: Get worker nodes
        shell: |
         {{ source_env_command }}
         system host-list --format value | grep worker | awk '{ print$2 }'
        register: worker_nodes_results
        until: worker_nodes_results is not failed

      - set_fact:
          compute_nodes: "{{ worker_nodes_results.stdout.replace('\n','#') }}"

      - name: Add standby controller and compute nodes to host lock/unlock list
        set_fact:
          lock_unlock_host_list: >-
            {{ ['controller-1'] + compute_nodes.split('#') if compute_nodes | length > 0
            else ['controller-1']}}

      - name: Lock standby controller and compute nodes
        shell: |
          {{ source_env_command }}
          system host-lock {{ item }} --force {{ cli_confirmation | default('--yes') }}
        register: host_lock_output
        failed_when: >-
          host_lock_output.stderr != "Avoiding lock action on already 'locked' host item"
          and host_lock_output.rc != 0
        loop: "{{ lock_unlock_host_list }}"

      - name: Wait until standby controller and compute nodes are locked
        shell: |
          {{ source_env_command }}
          system host-list --format value | grep {{ item }}
        register: host_list_var
        retries: 40
        delay: 15
        until: >-
          'stdout' in host_list_var
          and 'locked disabled' in host_list_var.stdout
        loop: "{{ lock_unlock_host_list }}"

      # The active controller on the subcloud will target other nodes and do certificate recovery
      - block:
        - name: >-
            Running certificate recovery on other nodes. Connect to the subcloud and run
            'tail -f /root/ansible.log' to follow the logs.
          command: >
            ansible-playbook
            /usr/share/ansible/stx-ansible/playbooks/recover_subcloud_certificates_other_nodes.yml
            -e "compute_nodes={{ compute_nodes }} ansible_ssh_user={{ ansible_ssh_user }}
            ansible_ssh_pass={{ ansible_ssh_pass }} ansible_become_pass={{ ansible_become_pass }}
            certificate_recovery_necessary={{ certificate_recovery_necessary }}" -v
          register: other_nodes_recovery_output
          no_log: true

        always:
          - name: Print the output of platform certificate recovery playbook
            debug:
              msg: >-
                {{ other_nodes_recovery_output.stdout if 'stdout' in other_nodes_recovery_output
                else 'Failed to run certificate recovery on other nodes.' }}

      - name: Unlock standby controller and compute nodes
        shell: |
          {{ source_env_command }}
          system host-unlock {{ item }}
        register: host_unlock_output
        retries: 20
        delay: 15
        until: host_unlock_output is not failed
        loop: "{{ lock_unlock_host_list }}"

      - name: Wait until standby controller and compute nodes reboots after host-unlock
        shell: |
          {{ source_env_command }}
          system host-list --format value | grep {{ item }}
        register: host_list_var
        retries: 40
        delay: 15
        until: >-
          'stdout' in host_list_var
          and 'unlocked enabled' in host_list_var.stdout
          and 'offline' not in host_list_var.stdout
        loop: "{{ lock_unlock_host_list }}"

      - name: Get storage nodes
        shell: |
          {{ source_env_command }}
          system host-list --format value | grep storage | awk '{ print$2 }'
        register: storage_nodes_results
        until: storage_nodes_results is not failed
      - name: Lock / Unlock storage nodes when they are present
        block:
          - name: Check if 2 storage nodes or more are present
            fail:
              msg: >-
                Subcloud in a bad state.
                Wrong number of storage nodes: {{ storage_nodes_results.stdout_lines | length }}
            when: "{{ storage_nodes_results.stdout_lines | length < 2 }}"

          - name: Host-lock / unlock storage nodes
            include_tasks: host-lock-unlock.yml
            loop: "{{ storage_nodes_results.stdout_lines }}"

        when: storage_nodes_results.stdout_lines | length > 0

      # 200.004: controller-0 experienced a service-affecting failure. Auto-recovery in progress.
      # Manual Lock and Unlock may be required if auto-recovery is unsuccessful.
      - name: Verify if controller-0 blocking management affecting alarm is present
        shell: |
          {{ source_env_command }}
          fm alarm-list --mgmt_affecting --nowrap | grep "200.004" | grep "controller-0"
        register: c0_service_affecting_alarm
        failed_when: c0_service_affecting_alarm.stderr | length > 0

      - set_fact:
          need_to_host_lock_unlock_c0: >-
            {{ true if c0_service_affecting_alarm.stdout | length > 0 else false }}

      - name: Do host-lock and unlock of controller-0 to clear up management affecting alarm
        block:
          - name: Trigger a host-swact out of controller-0
            shell: |
              {{ source_env_command }}
              system host-swact controller-0 {{ cli_confirmation | default('--yes') }}
            register: host_swact_var
            retries: 3
            delay: 15
            until: host_swact_var.stderr | length == 0

          - name: Wait until swact finishes
            shell: |
              {{ source_env_command }}
              system host-show controller-1
            register: host_show_var
            retries: 40
            delay: 15
            until: >-
              'stdout' in host_show_var
              and 'Controller-Active' in host_show_var.stdout

          - name: Host-lock / unlock of controller-0
            include_tasks: host-lock-unlock.yml
            loop: "{{ ['controller-0'] }}"

          - name: Trigger a host-swact out of controller-1
            shell: |
              {{ source_env_command }}
              system host-swact controller-1 {{ cli_confirmation | default('--yes') }}
            register: host_swact_var
            retries: 3
            delay: 15
            until: host_swact_var.stderr | length == 0

          - name: Wait until swact finishes
            shell: |
              {{ source_env_command }}
              system host-show controller-0
            register: host_show_var
            retries: 40
            delay: 15
            until: >-
              'stdout' in host_show_var
              and 'Controller-Active' in host_show_var.stdout

        when: need_to_host_lock_unlock_c0
    when:
      - system_mode_result.stdout == 'duplex'
      - is_active_controller
      # when running from enrollment playbook, only 1 node is provisioned
      - mode is not defined or mode != "enroll"

  - block:
    - name: Restart cert-mon and cert-alarm to clear up certificate related alarms
      shell: echo "cert-mon cert-alarm" | xargs -n1 sm-restart service
      become: yes

    - name: Wait until management affecting alarms clear up
      shell: |
        {{ source_env_command }}
        fm alarm-list --mgmt_affecting | grep -c True
      register: mgmt_affecting_present
      # Fail will be better handled by next task
      failed_when: false
      # Up to 10 mins to have some buffer, but usually takes ~ 3 mins.
      retries: 30
      delay: 20
      until:
        - mgmt_affecting_present.stdout | int == 0
        - mgmt_affecting_present.stderr | length == 0

    - name: Fail when management affecting alarms don't clear up
      fail:
        msg: >-
          The subcloud has management affecting alarms which are blocking the rehoming procedure
          from continuing. The subcloud may still be recoverable, connect to it and run
          'fm alarm-list --mgmt_affecting' to check the alarms. Please
          resolve the alarm condition(s) then try again.
      when: mgmt_affecting_present.stdout | int != 0 or mgmt_affecting_present.stderr | length > 0

    when: is_active_controller

  - name: Mark certificate recovery as complete
    block:
      - name: Delete {{ certificate_recovery_in_progress_flag }}
        file:
          path: "{{ certificate_recovery_in_progress_flag }}"
          state: absent

      - set_fact:
          certificate_recovery_complete: true

  when: certificate_recovery_necessary

  always:
    - name: Delete root ca key file after use in compute nodes
      file:
        path: "{{ k8s_root_ca_key }}"
        state: absent
      become: yes
      when: is_compute

---
#
# Copyright (c) 2019-2020 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# ROLE DESCRIPTION:
#   This role is to restore the remaining data in the backup tarball
#   during platform restore.
#

# These hieradata were generated after persist-config role was run. They
# will be re-generated when sysinv is restarted after postgres db is restored
- name: Remove newly generated hieradata data
  file:
    path: "{{ item }}"
    state: absent
  with_items:
    - "{{ puppet_permdir }}/hieradata/{{ controller_floating_address|ipmath(1) }}.yaml"
    - "{{ puppet_permdir }}/hieradata/system.yaml"
    - "{{ puppet_permdir }}/hieradata/secure_system.yaml"

# User postgres needs access files in this folder during restore
# Permissions will be set back to 0750 when host is unlocked
- name: Correct staging directory permissions for restore
  file:
    path: "{{ staging_dir }}"
    state: directory
    recurse: yes
    owner: root
    group: root
    mode: 0755

# To work around an ansible quirk that regex_replace filter
# is ignored when it is applied to variables in the command module
- name: Remove leading '/' from dir name
  set_fact:
    short_platform_conf_path: "{{ platform_conf_path | regex_replace('^\\/', '') }}"
    short_config_permdir: "{{ config_permdir | regex_replace('^\\/', '') }}"
    short_ceph_backend_flag: "{{ ceph_backend_flag | regex_replace('^\\/', '') }}"

- name: Extract platform.conf from the backup tarball
  command: >-
    tar -C {{ staging_dir }} -xpf {{ target_backup_dir }}/{{ backup_filename }}  --transform='s,.*/,,'
    {{ short_platform_conf_path }}/platform.conf
  args:
    warn: false

- name: Search for the new INSTALL_UUID in /etc/platform/platform.conf
  shell: grep INSTALL_UUID {{ platform_conf_path }}/platform.conf
  register: result

- name: Replace INSTALL_UUID with the new one
  lineinfile:
    dest: "{{ staging_dir }}/platform.conf"
    regexp: 'INSTALL_UUID'
    line: "{{ result.stdout }}"

- name: Strip out entries that are host specific
  lineinfile:
    dest: "{{ staging_dir }}/platform.conf"
    regexp: "{{ item }}"
    state: absent
  with_items:
    - '^oam_interface='
    - '^cluster_host_interface='
    - '^UUID='

- name: Search for the management_interface in /etc/platform/platform.conf
  shell: grep management_interface {{ platform_conf_path }}/platform.conf
  failed_when: false
  register: result

- name: Replace management_interface with the new one
  lineinfile:
    dest: "{{ staging_dir }}/platform.conf"
    regexp: '^management_interface='
    line: "{{ result.stdout }}"
  when: result.rc == 0

- name: Replace platform config file
  command: mv -f {{ staging_dir }}/platform.conf {{ platform_conf_path}}/platform.conf

# While licences are not enforced, STX offers support for them through the "system license-install"
# command. The licenses are stored in /etc/platform/.license and /opt/platform/config/<version>/.license
# It is good practice to support license restoration, even if they are not enforced.
- name: Check if license exists in backup config permdir (opt/platform/config)
  command: tar -tf {{ target_backup_dir }}/{{ backup_filename }} '{{ short_config_permdir }}/.license'
  register: check_permdir_license
  failed_when: false
  args:
    warn: false

- name: Restore license in config permdir (/opt/platform/config/...)
  command: >-
    tar -C {{ config_permdir }} -xpf {{ target_backup_dir }}/{{ backup_filename }}
    --overwrite --transform='s,.*/,,' '{{ short_config_permdir }}/.license'
  args:
    warn: false
  when: check_permdir_license.rc is defined and
        check_permdir_license.rc == 0

- name: Check if license exists in backup platform config (etc/platform)
  command: tar -tf {{ target_backup_dir }}/{{ backup_filename }} '{{ short_platform_conf_path }}/.license'
  register: check_platform_license
  failed_when: false
  args:
    warn: false

- name: Restore license in platform config (/etc/platform/)
  command: >-
    tar -C {{ platform_conf_path }} -xpf {{ target_backup_dir }}/{{ backup_filename }}
    --overwrite --transform='s,.*/,,' '{{ short_platform_conf_path }}/.license'
  args:
    warn: false
  when: check_platform_license.rc is defined and
        check_platform_license.rc == 0

# Restore resolv.conf and dnsmaq
- name: Extract resolv.conf from backup tarball
  command: >-
    tar -C /etc -xpf {{ target_backup_dir }}/{{ backup_filename }} --overwrite
    --transform='s,.*/,,' etc/resolv.conf
  args:
    warn: false

- name: Restore resolv.conf in config permdir (/opt/platform/config/...)
  command: >-
    tar -C {{ config_permdir }} -xpf {{ target_backup_dir }}/{{ backup_filename }}
    --overwrite --transform='s,.*/,,' '{{ short_config_permdir }}/resolv.conf'
  args:
    warn: false

- name: Restore dnsmaq in config permdir (/opt/platform/config/...)
  command: >-
    tar -C {{ config_permdir }} -xpf {{ target_backup_dir }}/{{ backup_filename }}
    --overwrite --transform='s,.*/,,' '{{ short_config_permdir }}/dnsmasq*'
  args:
    warn: false

- name: Remove leading '/' from directory name
  set_fact:
    short_pxe_config_permdir: "{{ pxe_config_permdir | regex_replace('^\\/', '') }}"

- name: Restore boot files in pxelinux.cfg dir
  command: >-
    tar -C {{ pxe_config_permdir }} -xpf {{ target_backup_dir }}/{{ backup_filename }}
    --overwrite --transform='s,.*/,,' '{{ short_pxe_config_permdir }}/*-*-*'
  args:
    warn: false

- name: Extract ldap.db to staging directory
  command: >-
    tar -C {{ staging_dir }} -xpf {{ target_backup_dir }}/{{ backup_filename }}
    --transform='s,.*/,,' '*/ldap.db'
  args:
    warn: false

- name: Stop openldap service
  shell: "export SYSTEMCTL_SKIP_REDIRECT=1; /etc/init.d/openldap stop"

- name: Delete ldap directory
  file:
    path: "{{ ldap_permdir }}"
    state: absent

- name: Recreate ldap directory
  file:
    path: "{{ ldap_permdir }}"
    state: directory
    recurse: yes
    owner: root
    group: root
    mode: 0755

- name: Restore ldap
  shell: slapadd -F /etc/openldap/schema -l {{ staging_dir }}/ldap.db

- name: Start openldap service
  shell: "export SYSTEMCTL_SKIP_REDIRECT=1; /etc/init.d/openldap start"

- name: Delete file from staging dir
  file:
    path: "{{ staging_dir }}/ldap.db"
    state: absent

- name: Restore home directory
  shell: tar -C / --overwrite -xpf {{ target_backup_dir }}/{{ backup_filename }} 'home/*'
  args:
    warn: false
  become_user: root

- name: Restore armada manifests and extension filesystem
  command: tar -C / --overwrite -xpf {{ target_backup_dir }}/{{ backup_filename }} {{ item }}
  args:
    warn: false
  become_user: root
  with_items:
    - "{{ armada_permdir | regex_replace('^\\/', '') }}"
    - "{{ extension_permdir | regex_replace('^\\/', '') }}"

- name: Restore sysinv default configuration file
  command: >-
    tar -C {{ sysinv_config_permdir }} -xpf {{ target_backup_dir }}/{{ backup_filename }}
    --transform='s,.*/,,' '*/sysinv.conf.default'
  args:
    warn: false

- name: Look for the flag indicating that ceph is configured
  shell: "tar -tf {{ target_backup_dir }}/{{ backup_filename }} | grep '{{ short_ceph_backend_flag }}'"
  args:
    warn: false
  failed_when: false
  register: ceph_backend

# Restore ceph crushmap if ceph backend is configured
- block:
  # Can't store ceph crushmap at sysinv_config_permdir (/opt/platform/sysinv/)
  # for AIO systems because when unlocking controller-0 for the first time,
  # the crushmap is set thru ceph puppet when /opt/platform is not mounted yet.
  # So for AIO systems store the crushmap at /etc/sysinv.
  - name: Set ceph crushmap directory to /etc/sysinv if it is AIO system
    set_fact:
      ceph_crushmap_dir: /etc/sysinv
    when: system_type == 'All-in-one'

  - name: Set ceph crushmap directory to /opt/platform/sysinv if it is non-AIO system
    set_fact:
      ceph_crushmap_dir: "{{ sysinv_config_permdir }}"
    when: system_type != 'All-in-one'

  - name: Restore ceph crush map
    command: >-
      tar -C {{ ceph_crushmap_dir }} -xpf {{ target_backup_dir }}/{{ backup_filename }}
      --transform='s,.*/,,' '*/crushmap.bin.backup'
    args:
      warn: false

  # Need to remove osd info from the crushmap before it is loaded into ceph.
  # When osds are created they will be inserted into the crushmap by ceph.
  # TODO: There might be a better command to do this, like the rebuild option
  # with the ceph-monstore-tool.
  - name: Remove osds from the crushmap
    shell: >-
      crushtool -i {{ ceph_crushmap_dir }}/{{ crushmap_file }} --tree |
      awk /osd/'{print $NF}' |
      xargs -i crushtool -i {{ ceph_crushmap_dir }}/{{ crushmap_file }} --remove-item {}
      -o {{ ceph_crushmap_dir }}/{{ crushmap_file }}

  when: ceph_backend.rc == 0

- name: Remove leading '/' from dc-vault directory
  set_fact:
    short_dc_vault_permdir: "{{ dc_vault_permdir | regex_replace('^\\/', '') }}"

- name: Look for dc-vault filesystem
  shell: "tar -tf {{ target_backup_dir }}/{{ backup_filename }} | grep '{{ dc_vault_permdir|basename }}'"
  args:
    warn: false
  failed_when: false
  register: search_result

- name: Restore dc-vault filesystem
  command: >-
    tar -C / --overwrite -xpf {{ target_backup_dir }}/{{ backup_filename }}
    {{ short_dc_vault_permdir }}
  args:
    warn: false
  when: search_result.rc == 0

- name: Remove leading '/' from deploy directory
  set_fact:
    short_deploy_permdir: "{{ deploy_permdir | regex_replace('^\\/', '') }}"

- name: Look for deploy files
  shell: "tar -tf {{ target_backup_dir }}/{{ backup_filename }} |
         grep {{ short_deploy_permdir }}"
  args:
    warn: false
  failed_when: false
  register: deploy_search_result

- name: Restore files in deploy directory (/opt/platform/deploy/...)
  command: >-
    tar -C / --overwrite -xpf {{ target_backup_dir }}/{{ backup_filename }}
    {{ short_deploy_permdir}}
  args:
    warn: false
  when: deploy_search_result.rc == 0

# TODO: Restore ceph_external when it is supported

- name: Create Helm overrides directory
  file:
    path: "{{ helm_overrides_permdir }}"
    state: directory
    recurse: yes
    owner: root
    group: root
    mode: 0755

- block:
  - name: Shutdown mtce
    command: /usr/lib/ocf/resource.d/platform/mtcAgent stop
    environment:
      OCF_ROOT: "/usr/lib/ocf"
      OCF_RESKEY_state: "active"

  - name: Stop services
    systemd:
      name: "{{ item }}"
      state: stopped
    with_items:
      - openstack-keystone
      - fminit
      - fm-api
      - sysinv-api
      - sysinv-conductor
      - sysinv-agent
      - openstack-barbican-api

  - name: Create staging directory for postgres data
    file:
      path: "{{ staging_dir }}/postgres"
      state: directory
      recurse: yes
      owner: root
      group: root
      mode: 0755

  - name: Extract postgres db to staging directory
    command: >-
      tar -C {{ staging_dir }}/postgres -xpf {{ target_backup_dir }}/{{ backup_filename }}
      --transform='s,.*/,,' '*/*\.postgreSql\.*'
    args:
      warn: false

  - name: Restore postgres db
    shell: "psql -f {{ item }} {{ (item|basename).split('.')[0] }}"
    become_user: postgres
    with_items:
      - "{{ staging_dir }}/postgres/postgres.postgreSql.config"
      - "{{ staging_dir }}/postgres/postgres.postgreSql.data"
      - "{{ staging_dir }}/postgres/template1.postgreSql.data"
      - "{{ staging_dir }}/postgres/sysinv.postgreSql.data"
      - "{{ staging_dir }}/postgres/keystone.postgreSql.data"
      - "{{ staging_dir }}/postgres/fm.postgreSql.data"
      - "{{ staging_dir }}/postgres/barbican.postgreSql.data"

  - name: Remove postgres staging directory
    file:
      path: "{{ staging_dir }}/postgres"
      state: absent

  # Set all the hosts including controller-0 to locked/disabled/offline state.
  # After the services are restarted, mtce will update controller-0 to
  # locked/disabled/online state. Setting controller-0 to offline state now
  # will ensure that keystone, sysinv and mtcAgent are indeed in-service after being restated.
  - name: Set all the hosts to locked/disabled/offline state
    shell: >-
      psql -c "update i_host set administrative='locked', operational='disabled',
      availability='offline'" sysinv
    become_user: postgres
    when: wipe_ceph_osds|bool and ceph_backend.rc == 0 or ceph_backend.rc != 0

  - name: Set all the hosts, except storage nodes to locked/disabled/offline state
    shell: >-
      psql -c "update i_host set administrative='locked', operational='disabled',
      availability='offline' where personality!='storage'" sysinv
    become_user: postgres
    when: not wipe_ceph_osds|bool and ceph_backend.rc == 0

  # Set platform-integ-apps to "uploaded" state, so that once ceph is up after
  # controller-0 is unlocked for the first time, the manifest will be applied.
  - name: Set platform-integ-apps to "uploaded" state
    shell: psql -c "update kube_app set status='uploaded' where name='platform-integ-apps'" sysinv
    become_user: postgres

  # If other applications are in "applied" state, set them to "uploaded" state to
  # avoid confusion. The apps will be brought up in stages after the
  # platform is restored.
  - name: Set applied apps to "uploaded" state and inactive
    shell: psql -c "update kube_app set status='uploaded',active='f' where status='applied'" sysinv
    become_user: postgres

  - name: Restart services
    systemd:
      name: "{{ item }}"
      state: restarted
    with_items:
      - openstack-keystone
      - fminit
      - fm-api
      - sysinv-api
      - sysinv-conductor
      - sysinv-agent
      - openstack-barbican-api

  - name: Bring up Maintenance Agent
    command: /usr/lib/ocf/resource.d/platform/mtcAgent start
    environment:
      OCF_ROOT: "/usr/lib/ocf"
      OCF_RESKEY_state: "active"

  - name: Wait for 90 secs before check if services come up
    wait_for: timeout=90

  # admin-keystone is always the very last to be ready,
  # So we just wait and check for admin-keystone to come up.
  - name: Make sure admin-keystone is ready
    shell: "ps -ef | grep admin-keystone | grep -v grep"
    register: result
    until: result.stdout.find("keystone") != -1
    retries: 6
    delay: 10

  # Run "system host-list" to verify that controller-0 is in
  # "online" state. This will ensure that keystone, sysinv and
  # mtcAgent are indeed in-service after being restated.
  - name: Check controller-0 is in online state
    shell: source /etc/platform/openrc; system host-show controller-0 --column availability --format value
    register: check_online
    failed_when: false
    retries: 30
    delay: 10
    until: check_online.stdout == "online"

  - name: Inform user that restore_platform is not successful
    debug:
      msg: >-
        Platform restore was unsuccessful. Please refer to the system administration
        guide for next step.
    when: check_online.stdout != "online"

  # Resize drbd-backed partitions
  - block:
    - name: Get DRBD-synced partitions
      shell: source /etc/platform/openrc; system controllerfs-list --format yaml
      register: controllerfs_partitions_output

    # First extend the logical volumes
    - name: Resize logical volumes (except database, dc-vault)
      command: "lvextend -L{{ item.size }}G /dev/cgts-vg/{{ item.logical_volume }}"
      failed_when: false
      with_items: "{{ controllerfs_partitions_output.stdout | from_yaml }}"
      when: item.name != "database" and item.name != "{{ dc_vault_permdir|basename }}"
      register: lvextend_output

    # In cases where we try to resize an LV to the size it already is, lvextend
    # will throw an error, but it's not an issue so we just ignore that error.
    - name: Fail if resize of logical volumes fail
      fail:
        msg: "{{ item.item }}  failed for the following reason: {{ item.stderr }}"
      when: item.rc is defined and item.rc != 0 and
            item.stderr is not search('matches existing size')
      with_items: "{{ lvextend_output.results }}"

    # The database LV is twice the size that is stored in sysinv (in order to support
    # upgrades), so we resize it seperately.
    - name: Resize database logical volume
      command: "lvextend -L{{ (item.size*2) }}G /dev/cgts-vg/{{ item.logical_volume }}"
      failed_when: false
      with_items: "{{ controllerfs_partitions_output.stdout | from_yaml }}"
      when: item.name == "database"
      register: lvextend_database_output

    - name: Fail if resize of database logical volume fails
      fail:
        msg: "{{ item.item }}  failed for the following reason: {{ item.stderr }}"
      when: item.rc is defined and item.rc != 0 and
            item.stderr is not search('matches existing size')
      with_items: "{{ lvextend_database_output.results }}"

    - name: Resize DRBD resources
      command: "drbdadm -- --assume-peer-has-space resize all"

    # Resize the filesystem on top of DRBD resources.
    # The information about which /dev/drbd<id> corresponds to each LV is hard-coded
    # in puppet and is not available in sysinv, so we provide a static list of devices
    # here as well.
    # Keep this list in sync with the device names specified in the stx-puppet repo at:
    # puppet-manifests/src/modules/platform/manifests/drbd.pp
    # NOTE: Only devices present in the "system controllerfs-list" command output
    #       need to be kept in sync. Filesystem that we don't allow resizing for
    #       (for example rabbitmq) or those that don't use the controllerfs
    #       command (for example cephmon) don't need to be kept in sync.
    - name: Resize DRBD filesystems
      command: "resize2fs {{ item }}"
      register: resize2fs_output
      failed_when: false
      with_items:
        - /dev/drbd0  # postgres
        - /dev/drbd2  # platform
        - /dev/drbd5  # extension
        - /dev/drbd7  # etcd
        - /dev/drbd8  # docker-distribution

    - name: Fail if resize of DRBD filesystems fail
      fail:
        msg: "{{ item.item }}  failed for the following reason: {{ item.stderr }}"
      when: item.rc != 0 and item.stderr is not search('Nothing to do!')
      with_items: "{{ resize2fs_output.results }}"
  # Restore ceph-mon data if ceph backend is configured
  - block:
    - block:
      # Recover procedure for systems with storage nodes is different from
      # that of systems with controller storage:
      # - For controller storage we recover ceph-mon data by scanning OSDs.
      # - For systems with storage nodes we get ceph-mon data from storage-0
      #   ceph-mon that is already up and will not be reinstalled.
      - name: Check if setup has storage nodes
        shell: source /etc/platform/openrc; system host-list --format value --column personality
        register: node_personalities
        failed_when: false

      # Get system_mode after restore and create flag file to skip wiping OSDs
      - name: Retrieve system mode
        shell: source /etc/platform/platform.conf; echo $system_mode
        register: restore_system_mode_result

      - name: Fail if system mode is not defined
        fail:
          msg: "system_mode is missing in /etc/platform/platform.conf"
        when: restore_system_mode_result.stdout_lines|length == 0

      - name: Set system mode fact
        set_fact:
          restore_system_mode: "{{ restore_system_mode_result.stdout_lines[0] }}"

      - name: Create flag file in /etc/platform to skip wiping OSDs
        file:
          path: "{{ skip_ceph_osds_wipe_flag }}"
          state: touch
        when: restore_system_mode != 'simplex'

      # Recover ceph data for systems with controller storage
      - include_role:
          name: recover-ceph-data
        when: node_personalities.stdout is not search('storage')

      - name: Mark crushmap as restored
        file:
          path: "{{ sysinv_config_permdir }}/.crushmap_applied"
          owner: root
          group: root
          mode: 644
          state: touch

      when: not wipe_ceph_osds|bool

    when: check_online.stdout == "online" and ceph_backend.rc == 0

  - name: Inform user that restore_platform is run successfully
    debug:
      msg: >-
        Controller-0 is now online. The next step is to unlock this controller.
        Please refer to the system administration guide for more details.
    when: check_online.stdout == "online"

  # Remove temporary staging area used by the copy module
  - name: Remove {{ ansible_remote_tmp }} directory
    file:
      path: "{{ ansible_remote_tmp }}"
      state: absent

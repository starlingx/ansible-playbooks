---
#
# Copyright (c) 2019-2024 Wind River Systems, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# ROLE DESCRIPTION:
#   This role is to backup StarlingX platform data and
#   StarlingX OpenStack Application data if the app exists.
#   The backup data are stored in two separate tar files.
#

- name: Normalize/Set default parameters
  set_fact:
    # Set parameters for ldap different paths by OS
    ldap_schema_path: "{{ '/etc/openldap/schema' if os_release == 'centos' else '/etc/ldap/schema' }}"
    backup_registry_filesystem_required: "{{ backup_registry_filesystem | bool }}"
    should_use_old_image_backup: "{{ backup_user_images|bool == true }}"
    include_hc_vault: "{{ backup_hc_vault | bool }}"
    omit_hc_vault: false


- name: Do StarlingX backup
  block:
    - name: Check if pigz package is installed
      block:

        - name: Issue command to pkg manager
          command: "{{ 'rpm -q' if os_release == 'centos' else 'dpkg -l' }} pigz"
          args:
            warn: false
          failed_when: false
          register: check

        - set_fact:
            pigz_check: "{{ 'succeeded' if check.rc == 0 else 'failed' }}"

      when: os_release in ["centos", "debian"]

    - name: Check if pigz package is installed
      package:
        name: pigz
        state: present
      check_mode: true
      register: pigz_check
      when: os_release not in ["centos", "debian"]

    - name: Check number of platform cores
      shell: |
        source /etc/platform/openrc
        system host-cpu-list $(hostname) --nowrap | grep " Platform " | wc -l
      register: num_platform_cores

    - name: Set compress program for backup tarball
      set_fact:
        compress_program: "{{ 'pigz' if num_platform_cores.stdout | int >= 4 and pigz_check is succeeded else 'gzip' }}"

    - name: Get kube-system default-registry-key
      command: >-
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get secret default-registry-key --namespace=kube-system
      failed_when: false
      register: kube_system_default_registry_key

    - name: Fail if there is no default-registry-key for kube-system
      fail:
        msg: "default-registry-key not found. Platform backup cannot proceed without it."
      when: kube_system_default_registry_key.rc != 0

    - name: Check Hashicorp vault status
      block:
      - name: Check if vault is applied
        shell: |
          source /etc/platform/openrc
          system application-show vault --format value --column status
        register: vault_applied_exists

      - name: Omit vault if status is empty or uploaded
        set_fact:
          include_hc_vault: false
          omit_hc_vault: true
        when: >-
          vault_applied_exists.stdout | length == 0 or
          vault_applied_exists.stdout == "uploaded"

      - name: Fail vault if status is not applied
        fail:
          msg: "Hashicorp vault application is {{ vault_applied_exists.stdout }}, not applied."
        when: vault_applied_exists.stdout != "applied"
      when: include_hc_vault | bool

    - name: Indicate if Hashicorp vault is omitted from status check
      debug:
        msg: "Hashicorp vault backup will be omitted because vault is not applied."
      when: omit_hc_vault | bool

    - name: Hashicorp vault precheck
      block:
      - name: Find vault manager pod
        shell: >-
          kubectl get pods -n vault | grep "vault-manager" | cut -d " " -f 1
        register: vault_manager_pod_name
        environment:
          KUBECONFIG: /etc/kubernetes/admin.conf

      - name: Fail if vault manager pod is not found
        fail:
          msg: "Vault manager pod is not found"
        when: vault_manager_pod_name.stdout | length == 0

      - name: Check vault system health
        shell: >-
          kubectl exec -n "vault" "{{ vault_manager_pod_name.stdout }}" --
          bash -c "source /opt/script/init.sh; snapshotPreCheck" 2>&1
        register: vault_system_health
        environment:
          KUBECONFIG: /etc/kubernetes/admin.conf

      - name: Fail if vault health check returns error
        fail:
          msg: "Vault system health check returned error"
        when: vault_system_health.rc != 0
      when: include_hc_vault | bool

    - name: Send application lifecycle notifications for pre-backup semantic check
      command: /usr/bin/sysinv-utils notify backup-semantic-check
      register: backup_semantic_check_notification_result
      failed_when: false

    - name: Fail if some application won't allow backup to proceed because semantic check failed.
      fail:
        msg: >
          Semantic check failed for backup action from application.
          {{ backup_semantic_check_notification_result.msg }}
      when: backup_semantic_check_notification_result.rc == 1

    - name: Fail if there is some other/internal error when sending lifecycle hook.
      fail:
        msg: >
          Failed to run backup-semantic-check action.
          {{ backup_semantic_check_notification_result.msg }}
      when: backup_semantic_check_notification_result.rc == 2

    - name: Send application lifecycle notifications for pre-backup action
      command: /usr/bin/sysinv-utils notify pre-backup-action
      register: pre_backup_notification_result
      failed_when: false

    - name: Fail if some application cannot handle the pre-backup action
      fail:
        msg: >
          Failed pre-backup action for application
          [{{ pre_backup_notification_result.msg }}]
      when: pre_backup_notification_result.rc != 0

    - name: Generate backup_in_progress alarm
      script: fm_alarm.py "--set" "--backup"
      register: alarm_result
      failed_when: false

    # Extracting all output lines from the script execution that
    # contain the keyword 'Exception'.
    - name: Fail if alarm script throws an exception
      fail:
        msg: >
          Failed to generate backup-in-progress alarm.
          [{{ alarm_result.stderr_lines | select('search', 'Exception') | join('\n') }}]
      when: alarm_result.rc != 0

    - name: Create temp dir
      tempfile:
        path: "{{ backup_dir }}"
        state: directory
      register: tempdir

    - name: Create a list of images found in local registry
      set_fact:
        system_image_list_path: "{{ tempdir.path }}/system_image_list.yaml"

    - name: Save local registry images currently on the system to list
      command: /usr/bin/sysinv-utils local-registry-list {{ system_image_list_path }} --all-apps

    - name: Create a list for cached local registry images
      set_fact:
        crictl_image_cache_list_path: "{{ tempdir.path }}/crictl_image_cache_list.txt"

    - name: Dump local registry images in crictl image cache to file
      shell:
        cmd: >
          crictl image ls
          | tail -n +2
          | awk '/registry.local:9001/ {print $1":"$2}'
          | sed 's$registry.local:9001/$$'
          | uniq
          | sort
          > '{{ crictl_image_cache_list_path }}'

    - name: Create postgres temp dir
      file:
        path: "{{ tempdir.path }}/postgres"
        state: directory
      register: postgres_dir

    - name: Create hashicorp vault temp dir
      file:
        path: "{{ tempdir.path }}/hc_vault_dir"
        state: directory
      register: hc_vault_dir
      when: include_hc_vault | bool

    - name: Backup roles, table spaces and schemas for databases.
      shell: >-
        sudo -u postgres pg_dumpall
        --clean --schema-only > {{ postgres_dir.path }}/postgres.postgreSql.config
      args:
        warn: false

    - name: Backup postgres, template1, sysinv, barbican db data
      shell: >-
        sudo -u postgres pg_dump --format=plain --inserts --disable-triggers --data-only
        {{ item }} > {{ postgres_dir.path }}/{{ item }}.postgreSql.data
      args:
        warn: false
      with_items:
        - postgres
        - template1
        - sysinv
        - barbican

    - name: Count subcloud related alarms
      command: >-
        psql -d fm -t -c "select count(*) from alarm
        where alarm_id like '280.%';"
      become_user: postgres
      register: subcloud_alarm_count

    - name: Backup subcloud related alarms
      block:
      - name: Copy subcloud alarms from postgres database to /tmp/.subcloud_alarm.csv
        command: >-
          psql -d fm -c "copy (select * from alarm where alarm_id like '280.%')
          to '/tmp/.subcloud_alarm.csv' with delimiter ';';"
        become_user: postgres

      - name: Move .subcloud_alarm.csv to postgres dir
        command: >-
          mv "/tmp/.subcloud_alarm.csv" {{ postgres_dir.path }}/.subcloud_alarm.csv
        become: yes
      when: subcloud_alarm_count.stdout | int > 0

    - name: Backup fm db data
      shell: >-
        sudo -u postgres pg_dump --format=plain --inserts --disable-triggers
        --data-only fm --exclude-table=alarm > {{ postgres_dir.path }}/fm.postgreSql.data
      args:
        warn: false

    - name: Backup keystone db data
      shell: >-
        sudo -u postgres pg_dump --format=plain --inserts --disable-triggers
        --data-only keystone > {{ postgres_dir.path }}/keystone.postgreSql.data
      args:
        warn: false

    - name: Check if it is dc controller
      command: >-
        grep -i "distributed_cloud_role\s*=\s*systemcontroller"
        {{ platform_conf_path }}/platform.conf
      register: check_dc_controller
      failed_when: false

    - block:
      - name: Backup dcmanager db for dc controller
        shell: >-
          sudo -u postgres pg_dump --format=plain --inserts --disable-triggers
          --data-only dcmanager > {{ postgres_dir.path }}/dcmanager.postgreSql.data
        args:
          warn: false

      - name: Backup dcorch db for dc controller
        set_fact:
          dcorch_db: "sudo -u postgres pg_dump --format=plain --inserts --disable-triggers --data-only dcorch "

      - name: Update dcorch tables that will be excluded from backup
        set_fact:
          dcorch_db: "{{ dcorch_db }} --exclude-table={{ item }}"
        with_items:
          - orch_job
          - orch_request
          - resource
          - subcloud_resource

      - name: Backup dcorch db
        shell: "{{ dcorch_db }} > {{ postgres_dir.path }}/dcorch.postgreSql.data"

      when: check_dc_controller.rc == 0

    - name: Create mariadb temp dir
      file:
        path: "{{ tempdir.path }}/mariadb"
        state: directory
      register: mariadb_dir

    - name: Check if mariadb pod is running
      shell: >-
        kubectl --kubeconfig={{ kube_config_dir }} get pods -n openstack | grep {{ mariadb_pod }} | grep -i 'running'
      failed_when: false
      register: check_mariadb_pod

    - block:
      - name: Set k8s cmd prefix
        set_fact:
          kube_cmd_prefix: "kubectl --kubeconfig={{ kube_config_dir }} exec -i {{ mariadb_pod }} -n openstack -- bash -c "

      - name: Show databases
        shell: "{{ kube_cmd_prefix }} 'exec mysql -uroot -p\"$MYSQL_DBADMIN_PASSWORD\" -e\"show databases\"'"
        register: databases

      - name: Backup mariadb
        shell: >-
          {{ kube_cmd_prefix }} 'exec mysqldump -uroot -p"$MYSQL_DBADMIN_PASSWORD" {{ item }}' >
          {{ mariadb_dir.path }}/{{ item }}.mariadb.data
        with_items: "{{ databases.stdout_lines | difference(skip_os_dbs) }}"

      when: check_mariadb_pod.rc == 0

    - block:

      - name: Get openstack application status
        shell: >-
          source /etc/platform/openrc; system application-show {{ openstack_app_name }} --column status --format value
        failed_when: false
        register: openstack_status

      - name: Fail the backup if MariaDB is not running
        fail:
          msg: "WARNING: {{ mariadb_pod }} is not running. Cannot backup mariadb data."
        when: openstack_status.stdout == "applied"
      when: check_mariadb_pod.rc != 0

    # In order to restore only the OpenStack application, the user needs to manually
    # remove, delete and upload the application.
    # This procedure will make the helm overrides data that is
    # present in the sysinv DB and the OpenStack application DB to be inconsistent.
    # A dump file containing the overrides data is used here to solve this inconsistency issue.
    # Note that this file will contain only 'update' sql commands because the
    # overrides are already created when we uploaded the application.
    - name: Create Helm overrides temp dir
      file:
        path: "{{ tempdir.path }}/helm_overrides_sqldump_dir"
        state: directory
      register: helm_overrides_sqldump_dir

    - name: Get the openstack Helm overrides from the from the database
      shell: >-
        psql -c "copy(select row_to_json(t) from (select name, user_overrides, system_overrides
        from helm_overrides where namespace='openstack') as t) to stdout" sysinv | sed -e 's/\\\\/\\/g'
      become_user: postgres
      register: helm_overrides_list

    - name: Generate postgres update commands for Helm overrides
      set_fact:
        updates_list: >
          {{ updates_list | default('') }}update helm_overrides set
          system_overrides={% if item.system_overrides %}'{{ item.system_overrides }}'{% else %}NULL{% endif %},
          user_overrides={% if item.user_overrides %}'{{ item.user_overrides }}'{% else %}NULL{% endif %}
          where name='{{ item.name }}' and namespace='openstack';
      with_items: "{{ helm_overrides_list.stdout_lines | map('from_json') | list }}"

    - name: Backup Helm overrides
      copy:
        dest: "{{ helm_overrides_sqldump_dir.path }}/helm_overrides_dump.sql"
        mode: 0755
        content: '{{ updates_list | default("") }}'

    # Check for protected backup directories in exclude_dirs options
    - block:
      - name: Set input exclude dirs list from exclude_dirs option
        set_fact:
          input_exclude_dirs:
            "{{ _input_exclude_dirs.split(',') | reject('equalto', '') | list }}"
        vars:
          _input_exclude_dirs: "{{ exclude_dirs | default(\"\") }}"

      - name: Fail if protected directories are present in exclude_dirs option
        fail:
          msg: |
            {{ item }} is protected, not to be used as exclude_dirs option
            Please use large directories that can be recreated e.g.
            /opt/dc-vault/backups or /opt/dc-vault/loads
        when: item in protected_backup_exclude_dirs
        with_items: "{{ input_exclude_dirs }}"

      when: exclude_dirs is defined

    - name: Set exclude targets
      set_fact:
        exclude_targets:
          "{{ backup.exclude + _exclude_dirs.split(',') | reject('equalto', '') | list }}"
      vars:
        _exclude_dirs: "{{ exclude_dirs | default(\"\") }}"

    - name: Exclude patches
      set_fact:
        exclude_targets: "{{ exclude_targets + [exclude_patches_glob] }}"
      when: exclude_patches|bool

    # Now Postgres data and MariaDB data are stored in staging dir, we can estimate
    # the disk size requirement for the backup archive.
    - name: Check the size (in KiB) of directories that will be backed up for platform
      shell: "du -sh -k  {{ item }} --exclude {{ exclude_targets|join(' --exclude ') }} | awk '{print $1}'"
      register: size_output_platform
      with_items: "{{ backup.targets }}"

    # Estimate the backup size. We add 128M overhead for things like ceph crushmap,
    # ldap data, etc. that will be generated and stored in the staging dir later on.
    - name: Estimate the total required disk size for platform backup archive
      set_fact:
        total_platform_size_estimation: "{{ total_platform_size_estimation|default(1024*128)|int + item.stdout|int }}"
      with_items: "{{ size_output_platform.results }}"
      loop_control:
        label: "{{ item.item }}"

    # Estimate registry filesystem backup size
    - name: Verify image registry filesystem backup size
      block:

        - name: Get the size of each image directories to be backed up (in KiB)
          shell: "du -sh -k  {{ item }} | awk '{print $1}'"
          register: size_output_registry_image_fs
          with_items: "{{ image_backup.targets }}"

        - name: Determine the free disk space requirement for image registry backup
          set_fact:
            total_platform_size_estimation: "{{ total_platform_size_estimation|default(1024*128)|int + item.stdout|int }}"
          with_items: "{{ size_output_registry_image_fs.results }}"
          loop_control:
            label: "{{ item.item }}"

      when: backup_registry_filesystem_required

    # For SystemController the dc-vault is part of platform but restored after controller-0 unlock
    # Create a separate archive for it
    - block:
        - name: Check the size (in KiB) of directories that will be backed up for dc-vault
          shell: "du -sh -k  {{ dc_vault_permdir }} | awk '{print $1}'"
          register: size_output_dc_vault

        - name: Estimate the total required disk size for platform backup archive
          set_fact:
            total_platform_size_estimation: "{{ total_platform_size_estimation|int + size_output_dc_vault.stdout|int }}"
      when: check_dc_controller.rc == 0

    - name: Check the free space in the archive dir
      shell: "df -k {{ backup_dir }} --output=avail | tail -1"
      register: df_output

    - name: Parse backup directory size
      set_fact:
        available_disk_size: "{{ df_output.stdout | int }}"

    - name: Fail if there is not enough free space in the archive dir to create platform backup
      fail:
        msg:
          Not enough free space in {{ backup_dir }}. It has {{ available_disk_size }}KiB.
          It needs at least {{ total_platform_size_estimation }}KiB.
      when: available_disk_size|int < total_platform_size_estimation|int

    - name: Estimate remaining space after reserving space for platform backup
      set_fact:
        remaining_disk_size_estimation: "{{ available_disk_size|int - total_platform_size_estimation|int }}"

    - block:
      # Estimate the disk size requirement for the OpenStack backup archive.
      - name: Check the size (in KiB) of directories that will be backed up for openstack
        shell: "du -sh -k  {{ item }} | awk '{print $1}'"
        with_items:
          - "{{ mariadb_dir.path }}"
          - "{{ helm_overrides_sqldump_dir.path }}"
          - "{{ fluxcd_permdir }}/{{ openstack_app_name }}"
          - "{{ helm_charts_permdir }}/starlingx"
        register: size_output_openstack

      # Estimate the OpenStack backup size.
      - name: Estimate the total required disk size for platform openstack archive
        set_fact:
          total_openstack_size_estimation: "{{ total_openstack_size_estimation|default(0)|int + item.stdout|int }}"
        with_items: "{{ size_output_openstack.results }}"
        loop_control:
          label: "{{ item.item }}"

      - name: Fail if there is not enough free space in the archive dir to create openstack backup
        fail:
          msg:
            Not enough free space in {{ backup_dir }}.
            Free space available is estimated to {{ remaining_disk_size_estimation }}KiB.
            It needs at least {{ total_openstack_size_estimation }}KiB.
        when: remaining_disk_size_estimation|int < total_openstack_size_estimation|int

      - name: Estimate remaining space after reserving space for openstack backup
        set_fact:
          remaining_disk_size_estimation: "{{ remaining_disk_size_estimation|int - total_openstack_size_estimation|int }}"

      when: check_mariadb_pod.rc == 0 or openstack_status.stdout == "uploaded"

    - name: Check if ldap LDIF configuration file exists
      stat:
        path: "{{ ldap_schema_path }}/cn=config.ldif"
      register: ldap_config

    - block:
      - name: Create ldap temp dir
        file:
          path: "{{ tempdir.path }}/ldap"
          state: directory
        register: ldap_dir

      - name: Name ldap db backup
        set_fact:
          ldap_db_backup: "{{ ldap_dir.path }}/ldap.db"

      - name: Backup LDAP DB
        command: "slapcat -d 0 -F {{ ldap_schema_path }} -l {{ ldap_db_backup }}"

      when: ldap_config.stat.exists

    - block:
      - name: Create ceph temp dir
        file:
          path: "{{ tempdir.path }}/ceph"
          state: directory
        register: ceph_dir

      - name: Name ceph crushmap backup
        set_fact:
          crushmap_file: "{{ ceph_dir.path }}/crushmap.bin.backup"

      - name: Create ceph crushmap backup
        command: "ceph osd getcrushmap -o {{ crushmap_file }}"

      - name: Register the content of etc/hostname from
        shell: cat /etc/hostname
        register: local_host

      - name: Set local_hostname
        set_fact:
          local_hostname: "{{ local_host.stdout_lines[0] | trim | default('') }}"

      # backup ceph.conf from ctrl-0 if we take backup from ctrl-1 on DX system
      - block:
        - name: Rename ceph.conf backup from controller-0
          set_fact:
            ceph_conf_ctrl_0: "{{ ceph_dir.path }}/ceph_controller-0.conf"

        - name: Copy ceph.conf from controller-0
          command: 'sshpass -p "{{ ansible_become_pass }}" scp -o StrictHostKeyChecking=no
                    sysadmin@controller-0:/etc/ceph/ceph.conf {{ ceph_conf_ctrl_0 }}'
          no_log: true

        when: (system_mode == 'duplex') and (local_hostname == "controller-1")

      when: ceph_backend.stat.exists

    - name: Send application lifecycle notifications for pre-etcd-backup action
      command: /usr/bin/sysinv-utils notify pre-etcd-backup-action
      register: pre_etcd_backup_notification_result
      failed_when: false

    - name: Fail if some application cannot handle the pre-etcd-backup action
      fail:
        msg: >
          Pre-etcd-backup action failed for application
          {{ pre_etcd_backup_notification_result.stderr }}.
      when: pre_etcd_backup_notification_result.rc != 0

    # Vault snapshot should be taken before the backup of etcd database.
    # A k8s secret is created that is associated with the snapshot.
    - name: Run hashicorp vault backupldap_schema_path
      block:
      - name: Include hashicorp vault backup role
        include_role:
          name: vault/vault_backup
        vars:
          vault_backup_dir: "{{ hc_vault_dir.path }}"
          vault_encrypt: false
          encrypt_hc_vault_secret: ""
          op_mode: "platform"

      - name: Find result files
        find:
          paths: "{{ hc_vault_dir.path }}"
          patterns: "hc-vault-snapshot-*.tar*"
        register: hc_vault_backup_result

      - name: Fail if incorrect number of file created from Hashicorp vault backup
        fail:
          msg: >
            There was an error with the Hashicorp vault backup process.
            Incorrect number of files produced.
        when: hc_vault_backup_result.matched != 2
      when: include_hc_vault | bool

    - name: Create etcd snapshot temp dir
      file:
        path: "{{ tempdir.path }}/etcd-snapshot"
        state: directory
      register: etcd_snapshot_dir

    - name: Name etcd snapshot backup
      set_fact:
        etcd_snapshot_file: "{{ etcd_snapshot_dir.path }}/etcd-snapshot.db"

    - name: Get etcd endpoints
      shell: |
        source /etc/platform/openrc
        system addrpool-list --nowrap | awk '/cluster-host-subnet/{print$14}'
      register: etcd_endpoint

    - name: Wrap etcd_endpoint in [] brackets if it's an ipv6 address
      set_fact:
        etcd_endpoint_parsed: "{{ etcd_endpoint.stdout | ipwrap }}"

    - name: Create etcd snapshot
      command: "etcdctl --endpoints https://{{ etcd_endpoint_parsed }}:2379 --cert=/etc/etcd/etcd-client.crt
                --key=/etc/etcd/etcd-client.key --cacert=/etc/etcd/ca.crt snapshot save {{ etcd_snapshot_file }}"
      environment:
        ETCDCTL_API: 3

    - name: Notify applications that etcd-backup succeeded
      command: /usr/bin/sysinv-utils notify post-etcd-backup-action success
      register: post_etcd_backup_notification_result
      failed_when: false

    - name: Fail if there is some other/internal error when sending lifecycle hook.
      set_fact:
        failure_msg: >
          "Failed to run post-etcd-backup action [{{ post_etcd_backup_notification_result.rc }}]"
      when: post_etcd_backup_notification_result.rc != 0

    - name: Create temp dir for override backup file
      file:
        path: "{{ tempdir.path }}/override"
        state: directory
      register: override_dir

    - name: Name override backup file
      set_fact:
        override_backup_file: "{{ override_dir.path }}/{{ host_override_backup_file }}"

    - name: Create the override backup file
      command: "/usr/bin/sysinv-utils create-host-overrides {{ override_backup_file }}"

    - block:
      - name: Get kubernetes_version from the DB
        shell: echo "select kubeadm_version from kube_cmd_versions" | psql -qAt -d sysinv
        register: kube_ver_select_result
        become_user: postgres

      - name: Set kubernetes_version to the value from DB
        set_fact:
          kubernetes_version: "{{ kube_ver_select_result.stdout_lines[0] }}"

      when: kubernetes_version is not defined

    - name: Get docker registries information
      include_role:
        name: common/push-docker-images
        tasks_from: get_docker_registries

    - name: Append registries configuration
      blockinfile:
        path: "{{ override_backup_file }}"
        marker: ""
        block: "{{ registries|default({}) | to_nice_yaml(indent=2) }}"

    - name: Use current timestamp as backups timestamp
      set_fact:
        backup_timestamp: "{{ lookup('pipe', 'date +%Y_%m_%d_%H_%M_%S') }}"

    - name: Attach timestamp to backups filename
      set_fact:
        platform_backup_file: "{{ platform_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"
        user_images_backup_file: "{{ user_images_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"
        openstack_backup_file: "{{ openstack_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"
        dc_vault_backup_file: "{{ dc_vault_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"
        hc_vault_backup_file: "{{ hc_vault_backup_filename_prefix }}_{{ backup_timestamp }}.tgz"

    - name: Set backup files absolute path
      set_fact:
        platform_backup_file_path: "{{ backup_dir }}/{{ platform_backup_file }}"
        user_images_backup_file_path: "{{ backup_dir }}/{{ user_images_backup_file }}"
        openstack_backup_file_path: "{{ backup_dir }}/{{ openstack_backup_file }}"
        dc_vault_backup_file_path: "{{ backup_dir }}/{{ dc_vault_backup_file }}"
        hc_vault_backup_file_path: "{{ backup_dir }}/{{ hc_vault_backup_file }}"

    - name: Save user uploaded images from local registry to an archive
      include_tasks: export-user-local-registry-images.yml
      vars:
        export_file_path: "{{ user_images_backup_file_path }}"
        kilo_free_size: "{{ remaining_disk_size_estimation }}"
      when: should_use_old_image_backup

    - name: Backup registry images filesystem
      include_tasks: backup-image-registry-filesystem.yml
      when: backup_registry_filesystem_required

    - name: Notify applications that backup succeeded
      command: /usr/bin/sysinv-utils notify post-backup-action success
      register: post_backup_notification_result
      failed_when: false

    - name: Fail if there is some other/internal error when sending lifecycle hook.
      fail:
        msg: "Failed to run post-backup action [{{ post_backup_notification_result.rc }}]"
      when: post_backup_notification_result.rc != 0

    # NOTE: Backup contents are defined on roles/backup/backup-system/vars/main.yml
    # Some additional files generated during this playbook's execution are appended to the backup targets here
    - name: Set fact for backup targets with extra files
      set_fact:
        final_backup_targets: "{{ (backup.targets + [
          etcd_snapshot_file,
          helm_overrides_sqldump_dir.path,
          override_backup_file,
          ldap_db_backup_,
          crushmap_file_,
          ceph_conf_ctrl_0_,
          system_image_list_path,
          crictl_image_cache_list_path,
          ]) | reject('equalto', '') | list }}"  # rejecting vars that might be empty
      vars:
        crushmap_file_: "{{ crushmap_file | default(\"\") }}"
        ceph_conf_ctrl_0_: "{{ ceph_conf_ctrl_0 | default(\"\") }}"
        ldap_db_backup_: "{{ ldap_db_backup | default(\"\") }}"

    # This is nasty to understand, but check the -vvv output to see what is going on
    # When we update the community.general collection from 1.3.6 to >=5.2.0, we can use "archive"
    # with exclusion_patterns
    - name: Create a tgz archive for platform backup
      shell: >-
        tar
        --use-compress-program={{ compress_program }}
        --exclude {{ exclude_targets | map('regex_replace', '^/', '') | list | join(' --exclude ') }}
        -cf {{ platform_backup_file_path }}
        $(ls -d
        {{ final_backup_targets | join(' ') }}
        2> /dev/null)
      args:
        warn: false
      # Changing the failed_when behavior to prevent the backup to fail on "file changed as we read it", which
      # makes tar return 1
      register: tar_cmd
      failed_when: tar_cmd.rc >= 2 or tar_cmd.rc < 0

    - name: Create a tgz archive for dc-vault backup
      shell: >-
        tar
        --use-compress-program={{ compress_program }}
        --exclude {{ exclude_targets | map('regex_replace', '^/', '') | list | join(' --exclude ') }}
        -cf {{ dc_vault_backup_file_path }}
        $(ls -d
        {{ dc_vault_permdir }}
        2> /dev/null)
      args:
        warn: false
      # Changing the failed_when behavior to prevent the backup to fail on "file changed as we read it", which
      # makes tar return 1
      register: tar_cmd
      failed_when: tar_cmd.rc >= 2 or tar_cmd.rc < 0
      when: check_dc_controller.rc == 0

    - name: Create a tgz archive for OpenStack backup
      shell: >-
        tar
        --use-compress-program={{ compress_program }}
        --exclude {{ exclude_targets | map('regex_replace', '^/', '') | list | join(' --exclude ') }}
        -cf {{ openstack_backup_file_path }}
        $(ls -d
        {{ fluxcd_permdir }}/{{ openstack_app_name }}
        {{ helm_charts_permdir }}/starlingx
        {{ mariadb_dir.path }}
        {{ helm_overrides_sqldump_dir.path }}
        2> /dev/null)
      args:
        warn: false
      # Changing the failed_when behavior to prevent the backup to fail on "file changed as we read it", which
      # makes tar return 1
      register: tar_cmd
      failed_when: tar_cmd.rc >= 2 or tar_cmd.rc < 0
      when: check_mariadb_pod.rc == 0 or openstack_status.stdout == "uploaded"

    - name: Create a tgz archive for Hashicorp vault backup
      shell: >-
        tar
        --use-compress-program={{ compress_program }}
        --exclude {{ exclude_targets | map('regex_replace', '^/', '')
        | list | join(' --exclude ') }}
        -cf {{ hc_vault_backup_file_path }}
        $(ls -d
        {{ hc_vault_dir.path }}
        2> /dev/null)
      args:
        warn: false
      # Changing the failed_when behavior to prevent the backup to fail on "file changed as we read it", which
      # makes tar return 1
      register: tar_cmd
      failed_when: tar_cmd.rc >= 2 or tar_cmd.rc < 0
      when: include_hc_vault | bool

    - name: Notify the user backup tar file(s) are available
      debug:
        msg: >-
          Backup tar file(s) are now available in {{ backup_dir }} on the active controller.

    - block:
      - name: Transfer platform backup tar file to the local machine
        synchronize:
          mode: pull
          src: "{{ platform_backup_file_path }}"
          dest: "{{ host_backup_dir }}/"
        register: backup_transfer
        until: backup_transfer.rc == 0
        retries: 3
        delay: 2
        become: false

      - name: Transfer openstack backup tar files to the local machine if it exists
        fetch:
          src: "{{ openstack_backup_file_path }}"
          dest: "{{ host_backup_dir }}/"
          flat: yes
        when: check_mariadb_pod.rc == 0 or openstack_status.stdout == "uploaded"
        no_log: true

      - name: Transfer for Hashicorp vault backup tar files to the local machine if it exists
        fetch:
          src: "{{ hc_vault_backup_file_path }}"
          dest: "{{ host_backup_dir }}/"
          flat: yes
        when: include_hc_vault | bool
        no_log: true

      # TODO transfer docker image archive which may be very big during remote play.
      # Fetch module fills the memory and has a very slow transfer rate due to base64 encoding
      # Maybe use synchronize module after upgrading ansible, backup-restore/transfer-file
      # role shows a github issue regarding why it can't be used now.

      - name: Notify the user where the backup tar file(s) can be found
        debug:
          msg: >-
            Backup tar file(s) have been transferred to {{ host_backup_dir }} on Ansible control host.
      when: inventory_hostname != 'localhost'

    - name: Find the backup tar files
      find:
        paths: "{{ backup_dir }}"
        patterns: '*backup_*.tgz'
      register: backup_files_perm_change

    - name: change permission of backup tar files
      file:
        path: "{{ item.path }}"
        owner: root
        group: root
        mode: 0640
      with_items: "{{ backup_files_perm_change.files }}"

  always:
    - name: Remove the temp dir
      file:
        path: "{{ tempdir.path }}"
        state: absent
      when: tempdir is defined and tempdir.path is defined

    - name: Remove the backup in progress flag file
      file:
        path: "{{ backup_in_progress_flag }}"
        state: absent

    - name: Clear backup_in_progress alarm
      script: fm_alarm.py "--clear" "--backup"
      register: alarm_result
      failed_when: false

    - name: Fail if alarm script throws an exception
      fail:
        msg: "Failed to clear backup-in-progress alarm."
      when: alarm_result.rc != 0

  rescue:
    - name: Notify applications that backup failed.
      command: /usr/bin/sysinv-utils notify post-backup-action failure
      failed_when: false

    - name: Set failure message playbook errors
      set_fact:
        failure_msg: >
          Failed task: {{ ansible_failed_task.name }}.
          Failed with error : {{ ansible_failed_result.msg }}
      when: ansible_failed_task is defined and ansible_failed_result is defined
      ignore_errors: true

    - name: Display failure message
      debug:
        msg: "{{ failure_msg }}"
      when: failure_msg is defined and failure_msg != ''

    - name: Force fail if this playbook or nested playbook failed
      fail:
        msg: >
          {% if failure_msg is defined and failure_msg != '' %}
          failure_msg: {{ failure_msg }}
          {% else %}
          failure_msg: Unable to backup the system. Please check ansible logs for the last failed task.
          {% endif %}
